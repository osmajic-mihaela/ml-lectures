{"cells":[{"cell_type":"markdown","metadata":{"id":"6tFocLFukR-3"},"source":["# Neuronske mreÅ¾e"]},{"cell_type":"markdown","metadata":{"id":"OutbHz5XkVKb"},"source":["### ğŸŒŸ Analogija    \n","*Zamislite da uÄite dete da prepoznaje pse:*\n","\n","OÄi primaju sliku.  \n","Mozak analizira detalje: uÅ¡i, rep, dlaku.   \n","Usta kaÅ¾u: 'Pas!'\n","\n","Neuronske mreÅ¾e rade isto â€“ primaju podatke, analiziraju kroz pregrÅ¡t veza i daju odgovor.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"ukGeKoTFh02Y"},"source":["# ğŸ“š Linearna funkcija y = ax + b"]},{"cell_type":"markdown","metadata":{"id":"Li1Cgxbmi3AD"},"source":["![img/4/l1.png](img/4/l1.png)\n","\n","\n","- Oslonite se samo na Äitanje sa grafa (bez raÄunanja), za x=3, koliko je y?  \n","\n","MoÅ¾emo da zakljuÄimo da svako x ima svoje y, ispravnije reÄeno funkcija preslikava x u y.\n","\n","  Posmatrajmo funkciju na drugaÄiji naÄin:   \n","  x -> ulaz   \n","  y -> izlaz   \n","  funkcija -> **neÅ¡to** Å¡to nam omoguÄ‡ava ispravnu konverziju x u y\n","\n","- Å ta ako funkcija ne omoguÄ‡ava ispravnu konverziju, ako za x vrednosti ne dobijamo Å¾eljene y vrednosti? -> Nije dobra funkcija.   \n","\n","- Å ta odreÄ‘uje izgled ove funkcije, a ujedno i rezultate izlaza? -> a i b parametri.\n","\n","- Kako moÅ¾emo da je popravimo, *naÅ¡timamo*? -> Popravljamo vrednosti parametara a i b kako bismo smanjili greÅ¡ku.\n","\n","![img/4/l2.png](img/4/l2.png)\n","\n","MoÅ¾emo da uvidimo da promenom parametra a menjamo nagib funkcije, dok parametrom b menjamo pomeraj.\n","\n","- Oslonite se ponovo samo na Äitanje sa grafa (bez raÄunanja), za x=3, koliko je y?\n","\n","- Å ta ako i dalje postoji greÅ¡ka u ovoj konverziji i funkcija ne obezbeÄ‘uje ispravne izlaze y za konkretne ulaze x? -> Popravljamo je dalje - traÅ¾imo parametre a i b koji Ä‡e smanjiti greÅ¡ku\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"lFABUooJhNgx"},"source":["# Kako tumaÄimo izvode?"]},{"cell_type":"markdown","metadata":{"id":"HQX_RAuOjR0u"},"source":["![img/4/2_func.jpeg](img/4/2_func.jpeg)\n","\n","- Posmatrajmo sliku.\n"," 1. Nalazimo se u vrednosti x=3, za x=3 -> y=5, ako se pomerimo za jako malu vrednost na x osi, recimo za 1, x=4 -> y=3. Vidimo da je doÅ¡lo do neke promene vrednosti izlaza,  \n"," > y1=5, y2=3 -> y2-y1=2.\n","\n"," 2. Nalazimo se u vrednosti x=3, za x=3 -> y=5, ako se pomerimo za jako malu vrednost na x osi, recimo za 1, x=4 -> y=1.\n"," > y1=5, y2=1 -> y2-y1=4.    \n","\n"," **ZakljuÄujemo da se u drugoj funkciji, za istu promenu ulaza x, drastiÄnije promenio izlaz y, u odnosu na prvu. MoÅ¾emo reÄ‡i da je druga funkcija \"osteljivija\" na promene ulaza.**\n","\n"," - Å ta je prvi izvod jedne funkcije u taÄki?\n"," > To je mera koliko brzo se menjaju njene vrednosti (izlazi) u odnosu na promenu vrednosti ulaza.\n","\n","---\n","\n","![img/4/extreme_values.png](img/4/extreme_values.png)\n","\n","- Osmotrite sliku. ZaÅ¡to smo raÄunali nulu prvog izvoda kada smo traÅ¾ili ekstreme neke funkcije?\n","\n","> Za malu promenu x vrednosti, y je ostao isti, promena je 0.    \n","To implicira da kada bismo mogli da izraÄunamo kad je promena 0 znali bismo da je to ekstrem funkcije (minimum ili maksimum). Iz tog razloga smo traÅ¾ili nulu prvog izvoda!\n","\n","- Å ta znaÄi kada je taj izvod pozitivan, a Å¡ta kada je negativan?\n","> Ako je izvod pozitivan -> PoveÄ‡anjem ulaznih vrednosti poveÄ‡avamo izlazne vrednosti -> funkcija raste.  \n","Ako je izvod negativan -> PoveÄ‡anjem ulaznih vrednosti smanjujemo izlazne vrednosti -> funkcija opada.\n","\n","Znak izvoda odreÄ‘uje kako se izlazi menjaju u odnosu na poveÄ‡anje promenljive (pozitivno/negativno).    \n","Vrednost izvoda odreÄ‘uje koliko brzo se izlazi menjaju u odnosu na promenu promenljive!\n","\n","---\n","\n","### **ğŸŒŸAnalogija**  \n","Zamislite da stojite na padini i Å¾elite da se spustite do dna. Izvodi su vaÅ¡ putokaz koji pokazuje:  \n","- **Gde je najstrmije** (veliki izvod = veÄ‡a strmina, brÅ¾e Ä‡ete se spustiti),  \n","- **U kom pravcu** (pozitivan izvod = ako nastavite idete uzbrdo, negativan = ako nastavite idete nizbrdo).  \n","\n","Ako je izvod pozitivan, Å¾elite da se vratite unazad (smanjujete x vrednost), ako je izvod negativan, Å¾elite da nastavite pravo (poveÄ‡avate x vrednost)\n","   "]},{"cell_type":"markdown","metadata":{"id":"AzBRyB7SjetS"},"source":["# Kako bismo mogli da iskoristimo izvode za taÅ¾enje ispravne linearne funkcije?"]},{"cell_type":"markdown","metadata":{"id":"l6zMbnL0jnTm"},"source":["### **Uvod**  \n","ZakljuÄili smo da, kada funkcija y = ax + b ne daje taÄne izlaze y za date ulaze x, potrebno je podesiti parametre a (nagib) i b (pomeraj) tako da oni smanje greÅ¡ku. Jedan naÄin podeÅ¡avanja leÅ¾i u **izvodima**, koji nam pokazuju kako parametri a i b utiÄu na greÅ¡ku izlaza - koliko i kako je promena greÅ¡ke osetljiva na promenu parametara a i b.\n","\n","Napomena: Obratite paÅ¾nju da ne posmatramo kako se izlaz menja u odnosu na parametre, veÄ‡ kako se **funkcija greÅ¡ke** menja!\n","\n","---\n","### **Korak 1: RaÄunanje izlaza**\n","Å½elimo da naÅ¡a funkcija preslikava x u Å¾eljeni y.    \n","Recimo da nam je za ulaz  $x = 3$, Å¾eljeni izlaz $y_{\\text{cilj}}$ = 7. Pukim  nagaÄ‘anjem pretpostavimo da je ispravna funkcija $y=1*x+2$. Da li smo u pravu?\n","\n","  $$y_{\\text{pred}} = 1*3+2$$\n","  $$y_{\\text{pred}} = 5$$   \n","\n","---\n","\n","### **Korak 2: DefiniÅ¡emo funkciju greÅ¡ke**  \n","Kvadrat razlike izmeÄ‘u Å¾eljenog i stvarnog izlaza je **greÅ¡ka**:  \n","\n","GreÅ¡ka $= (7-5)^2 = 4$ -> NaÅ¡a funkcija nije bila ispravna ğŸ˜\n","\n","Funkcija greÅ¡ke $= (y_{\\text{cilj}} - y_{\\text{pred}})^2 = (y_{\\text{cilj}} - (a*x+b))^2  $\n","\n","Napomena: U ovom momentu neÄ‡emo dokazivati i objaÅ¡njavati zaÅ¡to se koristi kvadrat greÅ¡ke, a ne samo razlika.\n","\n","---\n","\n","### **Korak 3: Izvodi nas vode do reÅ¡enja**\n","Å½elimo da smanjimo greÅ¡ku, raÄunamo izvode funkcije greÅ¡ke $L$ kako bismo shvatili koliko je ona osetljiva na promene parametara. Kada to budemo shvatili, aÅ¾uriraÄ‡emo parametre na pravi naÄin.\n","\n","1. **Izvod po \\( a \\)**:  \n","$$\n","\\frac{\\partial L}{\\partial a} = -2 * x * (y_{\\text{cilj}} - (a*x + b))\n","$$\n","Ovaj izvod pokazuje kako poveÄ‡anje \\( a \\) utiÄe na greÅ¡ku. Zamenimo vrednosti, x=3, $y_{\\text{cilj}}=7$, a=1, b=2.\n","\n","$$\n","\\frac{\\partial L}{\\partial a} = -2 * 3 * (7 - (1*3 + 2)) = -12\n","$$\n","\n","Izvod je negativan broj, Å¡to znaÄi da poveÄ‡anjem parametra a, smanjujemo greÅ¡ku!\n","\n","\n","2. **Izvod po \\( b \\)** (pomeraj):  \n","$$\n","\\frac{\\partial L}{\\partial b} = 2 * (y_{\\text{cilj}} - (a*x + b))\n","$$\n","Ovaj izvod pokazuje kako poveÄ‡anje \\( b \\) utiÄe na greÅ¡ku. Zamenimo vrednosti, x=3, $y_{\\text{cilj}}=7$, a=1, b=2.\n","\n","$$\n","\\frac{\\partial L}{\\partial b} = 2 * (7 - (1*3 + 2)) = 4\n","$$\n","\n","Izvod je pozitivan broj , Å¡to znaÄi da poveÄ‡anjem parametra b, poveÄ‡avamo greÅ¡ku!\n","\n","---\n","\n","### **Korak 4: AÅ¾uriranje parametara**  \n","Parametre a i b aÅ¾uriramo koristeÄ‡i izvode:  \n","$$\n","a_{\\text{novo}} = a_{\\text{staro}} - \\alpha * \\frac{\\partial L}{\\partial a}\n","$$\n","$$\n","b_{\\text{novo}} = b_{\\text{staro}} - \\alpha * \\frac{\\partial L}{\\partial b}\n","$$\n","\n","Imamo neke indikatore kako treba da menjamo paramtre kako bismo smanjili greÅ¡ku, ali ne Å¾elimo odmah da preduzimamo drastriÄne mere i menjamo ih za celu vrednost izvoda. Promenimo \"malo\" (korak 4) - izraÄunamo y (korak 1) - proverimo greÅ¡ku (korak 2) - raÄunamo izvode (korak 3) - promenimo \"malo\" (korak 4). Ponavljamo ovaj proces dok nismo minimizovali greÅ¡ku, idealno: dok greÅ¡ka ne postane jednaka 0.\n","\n","To koliko \"malo\" Å¾elimo da promenimo parametre predstavlja $ \\alpha$ - **brzina uÄenja**. Ako je  $ \\alpha$ = 0.05, promeniÄ‡emo parametre za 5% njihovog izvoda.\n","\n","Ubacimo vrednosti\n","$$\n","a_{\\text{novo}} = 1 - 0.05 * (-12) = 1.6\n","$$\n","\n","$$\n","b_{\\text{novo}} = 1 - 0.05 * 4 = 0.8\n","$$\n","\n","Ponovimo ceo ciklus - korak 1 - korak 2 - korak 3 - korak 4 dok ne minimizujemo greÅ¡ku do 0.\n","Ceo ciklus predstavlja proces obuÄavanja. I moÅ¾emo ga podeliti u dva procesa:\n","1. **Propagaciju unapred (Forward Propagation)** - Korak 1  \n","  - IzraÄunavamo izlaz modela $y_{\\text{cilj}} = w * x +b$   \n","2. **Propagaciju unazad (Backward propagation)** - Korak 2+3+4\n","  - RaÄunamo funkciju greÅ¡ke.\n","  - RaÄunamo izvode funkcije greÅ¡ke po parametrima.\n","  - AÅ¾uriramo parametre pomoÄ‡u gradijentnog spusta.\n","\n","**Gradijentni spust** (Gradient Descent) je deo propagacije unazad, taÄnije koraci 3 i 4. i predstavlja optimizaciju parametara a i b.\n","\n","---\n","\n","### ğŸŒ **Primer**  \n","1. PoÄetna funkcija: $ y = 1x + 2 $ â†’ za $ x = 3 $, $ y = 7 $.  \n","2. Å½eljeni izlaz: $ y_{\\text{cilj}} = 7 $.  \n","3. GreÅ¡ka: $ 7 - 5 = 2 $.  \n","4. IzraÄunajmo izvode:  \n","   - $ \\frac{\\partial \\text{L}}{\\partial a} = -2 * 3 * 2 = -12 $\n","   - $ \\frac{\\partial \\text{L}}{\\partial b} = 2 * 2 = 4 $  \n","5. Ako je ( $\\alpha $ =  0.05 ):  \n","   - $ a_{\\text{novo}} = 1 - 0.05 * (-12) = 1.6$\n","   - $ b_{\\text{novo}} = 1 -  0.05 * 4= 0.8$\n","\n","Nova funkcija: $ y = 1,6* x + 0.8 $. Za $ x = 3$, sada je $ y = 5.6 $, $greÅ¡ka= 1.4$, uspeli smo da smanjimo greÅ¡ku kroz samo jednu iteraciju (epohu)!\n","Postupak se ponavlja dok greÅ¡ka ne postane dovoljno mala.  \n","\n","Izvodima merimo **osetljivost greÅ¡ke** na promene parametara. KoriÅ¡Ä‡enjem ovih informacija, postupno smanjujemo greÅ¡ku, optimizujuÄ‡i parametre modela.   \n"]},{"cell_type":"markdown","metadata":{"id":"6gBUd2fcjqL8"},"source":["# ğŸ“š Neuron"]},{"cell_type":"markdown","metadata":{"id":"RYO64d9djthP"},"source":["> Linearna funkcija + Aktivaciona funkcija = Neuron !!!\n","\n","Izlaz linearne funkcije postaje ulaz u joÅ¡ jednu funkciju (aktivacionu funkciju - viÅ¡e o njima u nastavku, za sada je bitno da je to samo joÅ¡ jedna funkcija) i dobijamo neuron - osnovnu jedinicu graÄ‘e i funkcije neuronskih mreÅ¾a!\n","\n","- Parametar a se naziva **teÅ¾ina** (*weight*) i u literaturi je uglavnom predstavljen slovom w\n","- Parametar b se naziva **pomeraj** (*bias*)\n","\n","Proces optimizacije parametara ostaje apsolutno isti, samo Ä‡e izvod biti malo sloÅ¾eniji za raÄunanje. (Prisetite se izvoda sloÅ¾ene funkcije)\n","Linearna funkcija moÅ¾e da reÅ¡i samo linearne probleme, aktivaciona funkcija Äini neuron moÄ‡nijim jer uvodi nelinearnost i otvara polje reÅ¡avanja nelinearnih problema!\n","\n","---\n","\n","Å ta ako imamo viÅ¡e ulaza, a ne samo jedno x?   \n",">Linearna funkacija = $a_1*x_1 + a_2*x_2+...+b$    \n"," Linearna funkcija + Aktivaciona funkcija = Neuron   \n","\n","Linearna funkcija Ä‡e se izmeniti tako Å¡to Ä‡e svaki ulaz $x_n$ imati svoj parametar teÅ¾ine $a_n$.\n","Proces optimizacije parametara ostaje isti, samo Ä‡e raÄunanje biti malo duÅ¾e.\n","\n","---\n","\n","Kako je nastala ova ideja?\n","\n","Neuron je matematiÄki model inspirisan bioloÅ¡kim neuronima u mozgu. Njegova uloga je da primi ulazne podatke, obradi ih i proizvede izlaz. Ideja potiÄe iz Å¾elje da se simulira naÄin na koji neuroni u Å¾ivim organizmima prenose signale:\n","\n","1. Prima ulaze ($x_1$, $x_2$...),\n","2. Svakom ulazu dodeljuje \"vaÅ¾nost\" (teÅ¾ine $w_1$, $w_2$...), dodaje pomeraj (b) -> linearna funkcija\n","3. Prolazi kroz aktivacionu funkciju\n","4. GeneriÅ¡e izlaz koji moÅ¾e biti prosleÄ‘en drugim neuronima ili koriÅ¡Ä‡en kao konaÄan rezultat.\n","\n","![img/4/neuron_idea.png](img/4/neuron_idea.png)\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XM6ApFqFhH0o"},"source":["# ğŸ“š Neuronska mreÅ¾a"]},{"cell_type":"markdown","metadata":{"id":"J0glgEvlh7Y_"},"source":["### ğŸŒŸ Analogija    \n","*Zamislite da uÄite dete da prepoznaje pse:*\n","\n","OÄi primaju sliku.  \n","Mozak analizira detalje: uÅ¡i, rep, dlaku.   \n","Usta kaÅ¾u: 'Pas!'\n","\n","Neuronske mreÅ¾e rade isto â€“ primaju podatke, analiziraju kroz pregrÅ¡t veza i daju odgovor.\n","\n","---\n","\n","### ğŸ“š Struktura\n","\n","Jedan neuron je osnovna jedinica od koje sve poÄinje. On moÅ¾e da primi ulazne vrednosti, obradi ih pomoÄ‡u teÅ¾ina i pomeraja, primeni aktivacionu funkciju i proizvede izlaz. Ovo otkriÄ‡e je kljuÄno, ali jedan neuron sam po sebi moÅ¾e reÅ¡avati samo jednostavne probleme.\n","\n","Za sloÅ¾enije probleme, potreban nam je veÄ‡i broj neurona koji rade zajedno. Neuroni se ne koriste pojedinaÄno, veÄ‡ se grupiÅ¡u u slojeve. ViÅ¡e slojeva neurona Äini neuronsku mreÅ¾u. Na slici je prikazan viÅ¡eslojni perceptron (multilayer perceptron - MLP), ali postoje i mnoge druge arhitekture neuronske mreÅ¾e.\n","\n","U MLP-u, neuroni unutar istog sloja nisu povezani, svaki neuron nezavisno prima ulaze i obraÄ‘uje ih, ali nije direktno povezan sa drugim neuronima tog istog sloja. Neuroni iz jednog sloja su povezani sa neuronima iz sledeÄ‡eg sloja. To znaÄi da izlaz svakog neurona iz prethodnog sloja postaje ulaz neuronima u narednom sloju.\n","\n","---\n","\n","![img/4/nn_training.png](img/4/nn_training.png)\n"]},{"cell_type":"markdown","metadata":{"id":"0SwWaYsonkgG"},"source":["### ğŸ¯ KljuÄni koncepti"]},{"cell_type":"markdown","metadata":{"id":"zHG7uEGNns_l"},"source":["\n","Zamislite da uÄite da ubacujete loptu u koÅ¡ i posedujete 10 lopti.\n","#### 1. Loss funkcija   \n","##### ğŸŒŸ Analogija  \n","  Svaki put kada promaÅ¡ite, dobijate kritiku od trenera koju treba da ispravite. Å to ste bliÅ¾i koÅ¡u, manje stvari za ispravljanje.    \n","ğŸ‘‰**Loss funkcija** je trener â€“ meri koliko je model 'pogreÅ¡io' i kaÅ¾njava ga, a zatim mu pomaÅ¾e da poboljÅ¡a rezultat. Kao da proÄitate ceo udÅ¾benik jednom od korice do korice.\n","\n","##### ğŸ“š Definicija\n","Mera koja kvantifikuje koliko su predviÄ‘anja loÅ¡a.\n","\n","##### Primeri:\n","- MSE (Mean Squared Error): Za regresiju â€“ kaÅ¾njava velike greÅ¡ke.   \n","- Cross-Entropy: Za klasifikaciju â€“ meri razliku u verovatnoÄ‡ama.\n","\n","1. Model predvidi (npr. cena stana = 200.000â‚¬).   \n","2. Za loss funkciju smo izabrali MSE i izraÄunali razliku od stvarne vrednosti (npr. 210.000â‚¬ â†’ greÅ¡ka = 10.000â‚¬).   \n","3. Model koristi ovu greÅ¡ku da aÅ¾urira teÅ¾ine.   \n","\n","\n","---\n","\n","\n","#### 2. Epoha   \n","##### ğŸŒŸ Analogija\n","  Jedna epoha predstavlja ceo trening proces u kojem ste isprobali svih 10 lopti.    \n","ğŸ‘‰ U neuronskim mreÅ¾ama, jedna **epoha** znaÄi da je ceo skup podataka proÅ¡ao kroz model jednom.\n","\n","##### ğŸ“š Definicija\n","Jedan prolazak kroz sve primere iz trening skupa tokom treniranja.\n","\n","Modelu treba viÅ¡e epoha da \"uvidi Å¡iru sliku\" i smanji greÅ¡ku.   \n","  - PreviÅ¡e epoha: Rizik od overfitting-a (nauÄi napamet).    \n","  - Premalo epoha: Underfitting (nije nauÄio dovoljno).\n","\n","##### ğŸŒ Primer\n","Ako imate 1.000 slika, jedna epoha znaÄi da je model video svaku sliku taÄno jednom.\n","\n","---\n","\n","#### 3. Batch   \n","##### ğŸŒŸ Analogija\n","  Zamislite da ne Å¡utirate svih 10 lopti odjednom, veÄ‡ uzimate po 2 lopte, bacite ih, analizirate rezultate i onda bacate sledeÄ‡e 2.    \n","ğŸ‘‰ U modelima, batch je deo podataka koji prolazi kroz mreÅ¾u pre nego Å¡to se izvrÅ¡i aÅ¾uriranje parametara, batch size bi na prethodnom primeru bio 2.\n","\n","##### ğŸ“š Definicija\n","Podskup podataka unapred odreÄ‘ene velicine (Batch size) na osnovu kog se raÄuna srednja greÅ¡ka i aÅ¾uriraju parametri.  \n","\n","Batch size = 1 -> Parametri se menjaju na osnovu svakog primera, uÄenje je vrlo nestabilno    \n","Batch size = Ceo trening skup -> UÄenje stabilno, ali zahteva veliku memoriju\n","- Potrebno je naÄ‡i dobru veliÄinu u zavisnosti od koliÄine podataka, memorijskih resursa\n","\n","---\n","#### 4. Learning rate   \n","##### ğŸŒŸ Analogija\n","   Ako svaki put kada promaÅ¡ite, drastiÄno promenite tehniku Å¡uta, moÅ¾e se desiti da nikada ne naÄ‘ete pravi naÄin. Ako pravite samo male korekcije, moÅ¾da Ä‡ete se previÅ¡e sporo poboljÅ¡avati.     \n","ğŸ‘‰ Learning rate kontroliÅ¡e koliko brzo model menja svoje teÅ¾ine â€“ ako je previsok, model moÅ¾e da 'skaÄe' i ne nauÄi niÅ¡ta, ako je prenizak, moÅ¾e da uÄi presporo.\n","\n","##### ğŸ“š Definicija\n","Koliko *agresivno* model podeÅ¡ava teÅ¾ine nakon svake greÅ¡ke.\n","\n","- Prevelik: Model divergira, ne moÅ¾e da pronaÄ‘e minimum jer drastiÄno menja parametre.\n","- Premali: Treniranje traje veÄnost.\n","\n","---\n","\n","#### 5. Parametri i hiperparametri     \n","##### ğŸŒŸ Analogija    \n","  Parametri su vaÅ¡i miÅ¡iÄ‡i, pokreti i ugao Å¡uta â€“ oni se prilagoÄ‘avaju kako bi se poboljÅ¡ao rezultat.\n","  Hiperparametri su pravila koja birate pre nego Å¡to poÄnete da veÅ¾bate, poput broja lopti koje koristite, udaljenosti od koÅ¡a ili koliko brzo menjate tehniku.    \n","ğŸ‘‰ U modelima, parametri se automatski uÄe tokom treninga, dok hiperparametri odreÄ‘ujemo mi unapred (npr. broj epoha, veliÄina batch-a, learning rate).\n","\n","---\n","#### 6. Aktivacione funkcije (ekplodirajuci gradijenti, vanish gradijenti)\n","##### ğŸ“š Definicija\n"," Uvode nenelinearnost â€“ bez njih, neuronska mreÅ¾a bi bila obiÄna linearna regresija.\n","\n","##### ğŸŒ Primeri\n","ReLU: f(x) = max(0, x) â€“ popularna zbog jednostavnosti.     \n","Sigmoid: f(x) = 1 / (1 + e^-x) â€“ pretvara u verovatnoÄ‡u (0â€“1).      \n","Tanh: SliÄna sigmoidu, ali opseg -1 do 1.     \n","\n","\n","##### ğŸ› ï¸ Izazovi\n","1. NestajuÄ‡i Gradijenti (Vanishing Gradients)   \n","Gradijenti postaju toliko mali da se teÅ¾ine ne aÅ¾uriraju (naroÄito u dubokim mreÅ¾ama).   \n","Uzrok: Aktivacione funkcije kao sigmoid koje \"kompresuju\" izlaz u mali opseg.     \n","Jedno od reÅ¡enja: Koristiti ReLU ili njene varijante (Leaky ReLU).   \n","\n","2. EksplodirajuÄ‡i Gradijenti (Exploding Gradients)   \n","Gradijenti postaju ogromni, Å¡to dovodi do nestabilnog treniranja.    \n","Uzrok: Prevelike teÅ¾ine ili previsok learning rate.    \n","Jedno od reÅ¡enja: Gradient Clipping: OgraniÄi gradijente na maksimalnu vrednost.\n","PodeÅ¡avanje learning rate-a.\n"]},{"cell_type":"markdown","metadata":{"id":"ZTIklDcqidYo"},"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}