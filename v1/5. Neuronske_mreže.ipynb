{"cells":[{"cell_type":"markdown","metadata":{"id":"6tFocLFukR-3"},"source":["# Neuronske mre≈æe"]},{"cell_type":"markdown","metadata":{"id":"OutbHz5XkVKb"},"source":["### üåü Analogija    \n","*Zamislite da uƒçite dete da prepoznaje pse:*\n","\n","Oƒçi primaju sliku.  \n","Mozak analizira detalje: u≈°i, rep, dlaku.   \n","Usta ka≈æu: 'Pas!'\n","\n","Neuronske mre≈æe rade isto ‚Äì primaju podatke, analiziraju kroz pregr≈°t veza i daju odgovor.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"ukGeKoTFh02Y"},"source":["# üìö Linearna funkcija y = ax + b"]},{"cell_type":"markdown","metadata":{"id":"Li1Cgxbmi3AD"},"source":["![img/4/l1.png](img/4/l1.png)\n","\n","\n","- Oslonite se samo na ƒçitanje sa grafa (bez raƒçunanja), za x=3, koliko je y?  \n","\n","Mo≈æemo da zakljuƒçimo da svako x ima svoje y, ispravnije reƒçeno funkcija preslikava x u y.\n","\n","  Posmatrajmo funkciju na drugaƒçiji naƒçin:   \n","  x -> ulaz   \n","  y -> izlaz   \n","  funkcija -> **ne≈°to** ≈°to nam omoguƒáava ispravnu konverziju x u y\n","\n","- ≈†ta ako funkcija ne omoguƒáava ispravnu konverziju, ako za x vrednosti ne dobijamo ≈æeljene y vrednosti? -> Nije dobra funkcija.   \n","\n","- ≈†ta odreƒëuje izgled ove funkcije, a ujedno i rezultate izlaza? -> a i b parametri.\n","\n","- Kako mo≈æemo da je popravimo, *na≈°timamo*? -> Popravljamo vrednosti parametara a i b kako bismo smanjili gre≈°ku.\n","\n","![img/4/l2.png](img/4/l2.png)\n","\n","Mo≈æemo da uvidimo da promenom parametra a menjamo nagib funkcije, dok parametrom b menjamo pomeraj.\n","\n","- Oslonite se ponovo samo na ƒçitanje sa grafa (bez raƒçunanja), za x=3, koliko je y?\n","\n","- ≈†ta ako i dalje postoji gre≈°ka u ovoj konverziji i funkcija ne obezbeƒëuje ispravne izlaze y za konkretne ulaze x? -> Popravljamo je dalje - tra≈æimo parametre a i b koji ƒáe smanjiti gre≈°ku\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"lFABUooJhNgx"},"source":["# Kako tumaƒçimo izvode?"]},{"cell_type":"markdown","metadata":{"id":"HQX_RAuOjR0u"},"source":["![img/4/2_func.jpeg](img/4/2_func.jpeg)\n","\n","- Posmatrajmo sliku.\n"," 1. Nalazimo se u vrednosti x=3, za x=3 -> y=5, ako se pomerimo za jako malu vrednost na x osi, recimo za 1, x=4 -> y=3. Vidimo da je do≈°lo do neke promene vrednosti izlaza,  \n"," > y1=5, y2=3 -> y2-y1=2.\n","\n"," 2. Nalazimo se u vrednosti x=3, za x=3 -> y=5, ako se pomerimo za jako malu vrednost na x osi, recimo za 1, x=4 -> y=1.\n"," > y1=5, y2=1 -> y2-y1=4.    \n","\n"," **Zakljuƒçujemo da se u drugoj funkciji, za istu promenu ulaza x, drastiƒçnije promenio izlaz y, u odnosu na prvu. Mo≈æemo reƒái da je druga funkcija \"osteljivija\" na promene ulaza.**\n","\n"," - ≈†ta je prvi izvod jedne funkcije u taƒçki?\n"," > To je mera koliko brzo se menjaju njene vrednosti (izlazi) u odnosu na promenu vrednosti ulaza.\n","\n","---\n","\n","![img/4/extreme_values.png](img/4/extreme_values.png)\n","\n","- Osmotrite sliku. Za≈°to smo raƒçunali nulu prvog izvoda kada smo tra≈æili ekstreme neke funkcije?\n","\n","> Za malu promenu x vrednosti, y je ostao isti, promena je 0.    \n","To implicira da kada bismo mogli da izraƒçunamo kad je promena 0 znali bismo da je to ekstrem funkcije (minimum ili maksimum). Iz tog razloga smo tra≈æili nulu prvog izvoda!\n","\n","- ≈†ta znaƒçi kada je taj izvod pozitivan, a ≈°ta kada je negativan?\n","> Ako je izvod pozitivan -> Poveƒáanjem ulaznih vrednosti poveƒáavamo izlazne vrednosti -> funkcija raste.  \n","Ako je izvod negativan -> Poveƒáanjem ulaznih vrednosti smanjujemo izlazne vrednosti -> funkcija opada.\n","\n","Znak izvoda odreƒëuje kako se izlazi menjaju u odnosu na poveƒáanje promenljive (pozitivno/negativno).    \n","Vrednost izvoda odreƒëuje koliko brzo se izlazi menjaju u odnosu na promenu promenljive!\n","\n","---\n","\n","### **üåüAnalogija**  \n","Zamislite da stojite na padini i ≈æelite da se spustite do dna. Izvodi su va≈° putokaz koji pokazuje:  \n","- **Gde je najstrmije** (veliki izvod = veƒáa strmina, br≈æe ƒáete se spustiti),  \n","- **U kom pravcu** (pozitivan izvod = ako nastavite idete uzbrdo, negativan = ako nastavite idete nizbrdo).  \n","\n","Ako je izvod pozitivan, ≈æelite da se vratite unazad (smanjujete x vrednost), ako je izvod negativan, ≈æelite da nastavite pravo (poveƒáavate x vrednost)\n","   "]},{"cell_type":"markdown","metadata":{"id":"AzBRyB7SjetS"},"source":["# Kako bismo mogli da iskoristimo izvode za ta≈æenje ispravne linearne funkcije?"]},{"cell_type":"markdown","metadata":{"id":"l6zMbnL0jnTm"},"source":["### **Uvod**  \n","Zakljuƒçili smo da, kada funkcija y = ax + b ne daje taƒçne izlaze y za date ulaze x, potrebno je podesiti parametre a (nagib) i b (pomeraj) tako da oni smanje gre≈°ku. Jedan naƒçin pode≈°avanja le≈æi u **izvodima**, koji nam pokazuju kako parametri a i b utiƒçu na gre≈°ku izlaza - koliko i kako je promena gre≈°ke osetljiva na promenu parametara a i b.\n","\n","Napomena: Obratite pa≈ænju da ne posmatramo kako se izlaz menja u odnosu na parametre, veƒá kako se **funkcija gre≈°ke** menja!\n","\n","---\n","### **Korak 1: Raƒçunanje izlaza**\n","≈Ωelimo da na≈°a funkcija preslikava x u ≈æeljeni y.    \n","Recimo da nam je za ulaz  $x = 3$, ≈æeljeni izlaz $y_{\\text{cilj}}$ = 7. Pukim  nagaƒëanjem pretpostavimo da je ispravna funkcija $y=1*x+2$. Da li smo u pravu?\n","\n","  $$y_{\\text{pred}} = 1*3+2$$\n","  $$y_{\\text{pred}} = 5$$   \n","\n","---\n","\n","### **Korak 2: Defini≈°emo funkciju gre≈°ke**  \n","Kvadrat razlike izmeƒëu ≈æeljenog i stvarnog izlaza je **gre≈°ka**:  \n","\n","Gre≈°ka $= (7-5)^2 = 4$ -> Na≈°a funkcija nije bila ispravna üòû\n","\n","Funkcija gre≈°ke $= (y_{\\text{cilj}} - y_{\\text{pred}})^2 = (y_{\\text{cilj}} - (a*x+b))^2  $\n","\n","Napomena: U ovom momentu neƒáemo dokazivati i obja≈°njavati za≈°to se koristi kvadrat gre≈°ke, a ne samo razlika.\n","\n","---\n","\n","### **Korak 3: Izvodi nas vode do re≈°enja**\n","≈Ωelimo da smanjimo gre≈°ku, raƒçunamo izvode funkcije gre≈°ke $L$ kako bismo shvatili koliko je ona osetljiva na promene parametara. Kada to budemo shvatili, a≈æuriraƒáemo parametre na pravi naƒçin.\n","\n","1. **Izvod po \\( a \\)**:  \n","$$\n","\\frac{\\partial L}{\\partial a} = -2 * x * (y_{\\text{cilj}} - (a*x + b))\n","$$\n","Ovaj izvod pokazuje kako poveƒáanje \\( a \\) utiƒçe na gre≈°ku. Zamenimo vrednosti, x=3, $y_{\\text{cilj}}=7$, a=1, b=2.\n","\n","$$\n","\\frac{\\partial L}{\\partial a} = -2 * 3 * (7 - (1*3 + 2)) = -12\n","$$\n","\n","Izvod je negativan broj, ≈°to znaƒçi da poveƒáanjem parametra a, smanjujemo gre≈°ku!\n","\n","\n","2. **Izvod po \\( b \\)** (pomeraj):  \n","$$\n","\\frac{\\partial L}{\\partial b} = 2 * (y_{\\text{cilj}} - (a*x + b))\n","$$\n","Ovaj izvod pokazuje kako poveƒáanje \\( b \\) utiƒçe na gre≈°ku. Zamenimo vrednosti, x=3, $y_{\\text{cilj}}=7$, a=1, b=2.\n","\n","$$\n","\\frac{\\partial L}{\\partial b} = 2 * (7 - (1*3 + 2)) = 4\n","$$\n","\n","Izvod je pozitivan broj , ≈°to znaƒçi da poveƒáanjem parametra b, poveƒáavamo gre≈°ku!\n","\n","---\n","\n","### **Korak 4: A≈æuriranje parametara**  \n","Parametre a i b a≈æuriramo koristeƒái izvode:  \n","$$\n","a_{\\text{novo}} = a_{\\text{staro}} - \\alpha * \\frac{\\partial L}{\\partial a}\n","$$\n","$$\n","b_{\\text{novo}} = b_{\\text{staro}} - \\alpha * \\frac{\\partial L}{\\partial b}\n","$$\n","\n","Imamo neke indikatore kako treba da menjamo paramtre kako bismo smanjili gre≈°ku, ali ne ≈æelimo odmah da preduzimamo drastriƒçne mere i menjamo ih za celu vrednost izvoda. Promenimo \"malo\" (korak 4) - izraƒçunamo y (korak 1) - proverimo gre≈°ku (korak 2) - raƒçunamo izvode (korak 3) - promenimo \"malo\" (korak 4). Ponavljamo ovaj proces dok nismo minimizovali gre≈°ku, idealno: dok gre≈°ka ne postane jednaka 0.\n","\n","To koliko \"malo\" ≈æelimo da promenimo parametre predstavlja $ \\alpha$ - **brzina uƒçenja**. Ako je  $ \\alpha$ = 0.05, promeniƒáemo parametre za 5% njihovog izvoda.\n","\n","Ubacimo vrednosti\n","$$\n","a_{\\text{novo}} = 1 - 0.05 * (-12) = 1.6\n","$$\n","\n","$$\n","b_{\\text{novo}} = 1 - 0.05 * 4 = 0.8\n","$$\n","\n","Ponovimo ceo ciklus - korak 1 - korak 2 - korak 3 - korak 4 dok ne minimizujemo gre≈°ku do 0.\n","Ceo ciklus predstavlja proces obuƒçavanja. I mo≈æemo ga podeliti u dva procesa:\n","1. **Propagaciju unapred (Forward Propagation)** - Korak 1  \n","  - Izraƒçunavamo izlaz modela $y_{\\text{cilj}} = w * x +b$   \n","2. **Propagaciju unazad (Backward propagation)** - Korak 2+3+4\n","  - Raƒçunamo funkciju gre≈°ke.\n","  - Raƒçunamo izvode funkcije gre≈°ke po parametrima.\n","  - A≈æuriramo parametre pomoƒáu gradijentnog spusta.\n","\n","**Gradijentni spust** (Gradient Descent) je deo propagacije unazad, taƒçnije koraci 3 i 4. i predstavlja optimizaciju parametara a i b.\n","\n","---\n","\n","### üåç **Primer**  \n","1. Poƒçetna funkcija: $ y = 1x + 2 $ ‚Üí za $ x = 3 $, $ y = 7 $.  \n","2. ≈Ωeljeni izlaz: $ y_{\\text{cilj}} = 7 $.  \n","3. Gre≈°ka: $ 7 - 5 = 2 $.  \n","4. Izraƒçunajmo izvode:  \n","   - $ \\frac{\\partial \\text{L}}{\\partial a} = -2 * 3 * 2 = -12 $\n","   - $ \\frac{\\partial \\text{L}}{\\partial b} = 2 * 2 = 4 $  \n","5. Ako je ( $\\alpha $ =  0.05 ):  \n","   - $ a_{\\text{novo}} = 1 - 0.05 * (-12) = 1.6$\n","   - $ b_{\\text{novo}} = 1 -  0.05 * 4= 0.8$\n","\n","Nova funkcija: $ y = 1,6* x + 0.8 $. Za $ x = 3$, sada je $ y = 5.6 $, $gre≈°ka= 1.4$, uspeli smo da smanjimo gre≈°ku kroz samo jednu iteraciju (epohu)!\n","Postupak se ponavlja dok gre≈°ka ne postane dovoljno mala.  \n","\n","Izvodima merimo **osetljivost gre≈°ke** na promene parametara. Kori≈°ƒáenjem ovih informacija, postupno smanjujemo gre≈°ku, optimizujuƒái parametre modela.   \n"]},{"cell_type":"markdown","metadata":{"id":"6gBUd2fcjqL8"},"source":["# üìö Neuron"]},{"cell_type":"markdown","metadata":{"id":"RYO64d9djthP"},"source":["> Linearna funkcija + Aktivaciona funkcija = Neuron !!!\n","\n","Izlaz linearne funkcije postaje ulaz u jo≈° jednu funkciju (aktivacionu funkciju - vi≈°e o njima u nastavku, za sada je bitno da je to samo jo≈° jedna funkcija) i dobijamo neuron - osnovnu jedinicu graƒëe i funkcije neuronskih mre≈æa!\n","\n","- Parametar a se naziva **te≈æina** (*weight*) i u literaturi je uglavnom predstavljen slovom w\n","- Parametar b se naziva **pomeraj** (*bias*)\n","\n","Proces optimizacije parametara ostaje apsolutno isti, samo ƒáe izvod biti malo slo≈æeniji za raƒçunanje. (Prisetite se izvoda slo≈æene funkcije)\n","Linearna funkcija mo≈æe da re≈°i samo linearne probleme, aktivaciona funkcija ƒçini neuron moƒánijim jer uvodi nelinearnost i otvara polje re≈°avanja nelinearnih problema!\n","\n","---\n","\n","≈†ta ako imamo vi≈°e ulaza, a ne samo jedno x?   \n",">Linearna funkacija = $a_1*x_1 + a_2*x_2+...+b$    \n"," Linearna funkcija + Aktivaciona funkcija = Neuron   \n","\n","Linearna funkcija ƒáe se izmeniti tako ≈°to ƒáe svaki ulaz $x_n$ imati svoj parametar te≈æine $a_n$.\n","Proces optimizacije parametara ostaje isti, samo ƒáe raƒçunanje biti malo du≈æe.\n","\n","---\n","\n","Kako je nastala ova ideja?\n","\n","Neuron je matematiƒçki model inspirisan biolo≈°kim neuronima u mozgu. Njegova uloga je da primi ulazne podatke, obradi ih i proizvede izlaz. Ideja potiƒçe iz ≈æelje da se simulira naƒçin na koji neuroni u ≈æivim organizmima prenose signale:\n","\n","1. Prima ulaze ($x_1$, $x_2$...),\n","2. Svakom ulazu dodeljuje \"va≈ænost\" (te≈æine $w_1$, $w_2$...), dodaje pomeraj (b) -> linearna funkcija\n","3. Prolazi kroz aktivacionu funkciju\n","4. Generi≈°e izlaz koji mo≈æe biti prosleƒëen drugim neuronima ili kori≈°ƒáen kao konaƒçan rezultat.\n","\n","![img/4/neuron_idea.png](img/4/neuron_idea.png)\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XM6ApFqFhH0o"},"source":["# üìö Neuronska mre≈æa"]},{"cell_type":"markdown","metadata":{"id":"J0glgEvlh7Y_"},"source":["### üåü Analogija    \n","*Zamislite da uƒçite dete da prepoznaje pse:*\n","\n","Oƒçi primaju sliku.  \n","Mozak analizira detalje: u≈°i, rep, dlaku.   \n","Usta ka≈æu: 'Pas!'\n","\n","Neuronske mre≈æe rade isto ‚Äì primaju podatke, analiziraju kroz pregr≈°t veza i daju odgovor.\n","\n","---\n","\n","### üìö Struktura\n","\n","Jedan neuron je osnovna jedinica od koje sve poƒçinje. On mo≈æe da primi ulazne vrednosti, obradi ih pomoƒáu te≈æina i pomeraja, primeni aktivacionu funkciju i proizvede izlaz. Ovo otkriƒáe je kljuƒçno, ali jedan neuron sam po sebi mo≈æe re≈°avati samo jednostavne probleme.\n","\n","Za slo≈æenije probleme, potreban nam je veƒái broj neurona koji rade zajedno. Neuroni se ne koriste pojedinaƒçno, veƒá se grupi≈°u u slojeve. Vi≈°e slojeva neurona ƒçini neuronsku mre≈æu. Na slici je prikazan vi≈°eslojni perceptron (multilayer perceptron - MLP), ali postoje i mnoge druge arhitekture neuronske mre≈æe.\n","\n","U MLP-u, neuroni unutar istog sloja nisu povezani, svaki neuron nezavisno prima ulaze i obraƒëuje ih, ali nije direktno povezan sa drugim neuronima tog istog sloja. Neuroni iz jednog sloja su povezani sa neuronima iz sledeƒáeg sloja. To znaƒçi da izlaz svakog neurona iz prethodnog sloja postaje ulaz neuronima u narednom sloju.\n","\n","---\n","\n","![img/4/nn_training.png](img/4/nn_training.png)\n"]},{"cell_type":"markdown","metadata":{"id":"0SwWaYsonkgG"},"source":["### üéØ Kljuƒçni koncepti"]},{"cell_type":"markdown","metadata":{"id":"zHG7uEGNns_l"},"source":["\n","Zamislite da uƒçite da ubacujete loptu u ko≈° i posedujete 10 lopti.\n","#### 1. Loss funkcija   \n","##### üåü Analogija  \n","  Svaki put kada proma≈°ite, dobijate kritiku od trenera koju treba da ispravite. ≈†to ste bli≈æi ko≈°u, manje stvari za ispravljanje.    \n","üëâ**Loss funkcija** je trener ‚Äì meri koliko je model 'pogre≈°io' i ka≈ænjava ga, a zatim mu poma≈æe da pobolj≈°a rezultat. Kao da proƒçitate ceo ud≈æbenik jednom od korice do korice.\n","\n","##### üìö Definicija\n","Mera koja kvantifikuje koliko su predviƒëanja lo≈°a.\n","\n","##### Primeri:\n","- MSE (Mean Squared Error): Za regresiju ‚Äì ka≈ænjava velike gre≈°ke.   \n","- Cross-Entropy: Za klasifikaciju ‚Äì meri razliku u verovatnoƒáama.\n","\n","1. Model predvidi (npr. cena stana = 200.000‚Ç¨).   \n","2. Za loss funkciju smo izabrali MSE i izraƒçunali razliku od stvarne vrednosti (npr. 210.000‚Ç¨ ‚Üí gre≈°ka = 10.000‚Ç¨).   \n","3. Model koristi ovu gre≈°ku da a≈æurira te≈æine.   \n","\n","\n","---\n","\n","\n","#### 2. Epoha   \n","##### üåü Analogija\n","  Jedna epoha predstavlja ceo trening proces u kojem ste isprobali svih 10 lopti.    \n","üëâ U neuronskim mre≈æama, jedna **epoha** znaƒçi da je ceo skup podataka pro≈°ao kroz model jednom.\n","\n","##### üìö Definicija\n","Jedan prolazak kroz sve primere iz trening skupa tokom treniranja.\n","\n","Modelu treba vi≈°e epoha da \"uvidi ≈°iru sliku\" i smanji gre≈°ku.   \n","  - Previ≈°e epoha: Rizik od overfitting-a (nauƒçi napamet).    \n","  - Premalo epoha: Underfitting (nije nauƒçio dovoljno).\n","\n","##### üåç Primer\n","Ako imate 1.000 slika, jedna epoha znaƒçi da je model video svaku sliku taƒçno jednom.\n","\n","---\n","\n","#### 3. Batch   \n","##### üåü Analogija\n","  Zamislite da ne ≈°utirate svih 10 lopti odjednom, veƒá uzimate po 2 lopte, bacite ih, analizirate rezultate i onda bacate sledeƒáe 2.    \n","üëâ U modelima, batch je deo podataka koji prolazi kroz mre≈æu pre nego ≈°to se izvr≈°i a≈æuriranje parametara, batch size bi na prethodnom primeru bio 2.\n","\n","##### üìö Definicija\n","Podskup podataka unapred odreƒëene velicine (Batch size) na osnovu kog se raƒçuna srednja gre≈°ka i a≈æuriraju parametri.  \n","\n","Batch size = 1 -> Parametri se menjaju na osnovu svakog primera, uƒçenje je vrlo nestabilno    \n","Batch size = Ceo trening skup -> Uƒçenje stabilno, ali zahteva veliku memoriju\n","- Potrebno je naƒái dobru veliƒçinu u zavisnosti od koliƒçine podataka, memorijskih resursa\n","\n","---\n","#### 4. Learning rate   \n","##### üåü Analogija\n","   Ako svaki put kada proma≈°ite, drastiƒçno promenite tehniku ≈°uta, mo≈æe se desiti da nikada ne naƒëete pravi naƒçin. Ako pravite samo male korekcije, mo≈æda ƒáete se previ≈°e sporo pobolj≈°avati.     \n","üëâ Learning rate kontroli≈°e koliko brzo model menja svoje te≈æine ‚Äì ako je previsok, model mo≈æe da 'skaƒçe' i ne nauƒçi ni≈°ta, ako je prenizak, mo≈æe da uƒçi presporo.\n","\n","##### üìö Definicija\n","Koliko *agresivno* model pode≈°ava te≈æine nakon svake gre≈°ke.\n","\n","- Prevelik: Model divergira, ne mo≈æe da pronaƒëe minimum jer drastiƒçno menja parametre.\n","- Premali: Treniranje traje veƒçnost.\n","\n","---\n","\n","#### 5. Parametri i hiperparametri     \n","##### üåü Analogija    \n","  Parametri su va≈°i mi≈°iƒái, pokreti i ugao ≈°uta ‚Äì oni se prilagoƒëavaju kako bi se pobolj≈°ao rezultat.\n","  Hiperparametri su pravila koja birate pre nego ≈°to poƒçnete da ve≈æbate, poput broja lopti koje koristite, udaljenosti od ko≈°a ili koliko brzo menjate tehniku.    \n","üëâ U modelima, parametri se automatski uƒçe tokom treninga, dok hiperparametri odreƒëujemo mi unapred (npr. broj epoha, veliƒçina batch-a, learning rate).\n","\n","---\n","#### 6. Aktivacione funkcije (ekplodirajuci gradijenti, vanish gradijenti)\n","##### üìö Definicija\n"," Uvode nenelinearnost ‚Äì bez njih, neuronska mre≈æa bi bila obiƒçna linearna regresija.\n","\n","##### üåç Primeri\n","ReLU: f(x) = max(0, x) ‚Äì popularna zbog jednostavnosti.     \n","Sigmoid: f(x) = 1 / (1 + e^-x) ‚Äì pretvara u verovatnoƒáu (0‚Äì1).      \n","Tanh: Sliƒçna sigmoidu, ali opseg -1 do 1.     \n","\n","\n","##### üõ†Ô∏è Izazovi\n","1. Nestajuƒái Gradijenti (Vanishing Gradients)   \n","Gradijenti postaju toliko mali da se te≈æine ne a≈æuriraju (naroƒçito u dubokim mre≈æama).   \n","Uzrok: Aktivacione funkcije kao sigmoid koje \"kompresuju\" izlaz u mali opseg.     \n","Jedno od re≈°enja: Koristiti ReLU ili njene varijante (Leaky ReLU).   \n","\n","2. Eksplodirajuƒái Gradijenti (Exploding Gradients)   \n","Gradijenti postaju ogromni, ≈°to dovodi do nestabilnog treniranja.    \n","Uzrok: Prevelike te≈æine ili previsok learning rate.    \n","Jedno od re≈°enja: Gradient Clipping: Ograniƒçi gradijente na maksimalnu vrednost.\n","Pode≈°avanje learning rate-a.\n"]},{"cell_type":"markdown","metadata":{"id":"ZTIklDcqidYo"},"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}