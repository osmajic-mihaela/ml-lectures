{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X06B2GHlfgUd"
      },
      "source": [
        "# Nenadgledano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2AlpLvuvj_w"
      },
      "source": [
        "U nenadgledanom učenju model se trenira na osnovu x vrednosti primera, bez pratećih izlaza y. Cilj je da se otkrije „skrivena“ struktura u podacima, odnosno da se nauči reprezentacija koja opisuje unutrašnje odnose među podacima. Ovakav pristup može pomoći u identifikaciji obrazaca i veza u okviru skupa podataka. Pošto nema poznatih izlaza, ne postoji jedinstven standard za merenje performansi modela. \n",
        "Postupci se dele na:\n",
        "* Klasterizacija/klasterovanje  - grupisanje podataka na osnovu sličnosti\n",
        "* Redukcija dimenzionalnosti  - otkrivanje najdeskriptivnijih osobina podataka\n",
        "* Asocijativna pravila  - pronalaženje uobičajenih kombinacija osobina podataka\n",
        "\n",
        "\n",
        "![img/2/unsupervised-lr.jpg](img/2/unsupervised-lr.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bx59rWEvT6u"
      },
      "source": [
        "# Klasterovanje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44e-isyrvWss"
      },
      "source": [
        "### 📚 Definicija\n",
        "\n",
        "Klasterovanje je tip nenadgledanog učenja koji se koristi za grupisanje međusobno sličnih podataka.\n",
        "\n",
        "Često se koristi za redukciju dimenzionalnosti i detekciju autlajera.\n",
        "\n",
        "---\n",
        "\n",
        "## K Means\n",
        "\n",
        "K-means spada u ne-hijerarhijske metode grupisanja, što znači da unapred definiše broj grupa (klastera) i pokušava da dodeli podatke tim grupama na osnovu njihove međusobne sličnosti. Ovaj algoritam se često koristi u okviru procesa eksplorativne analize podataka, kada želimo da bolje razumemo strukturu skupa podataka, bez prethodnog znanja o tome kako bi oni trebalo da budu raspoređeni.\n",
        "\n",
        "⚙️ Pseudo-kod:\n",
        "\n",
        "```\n",
        "za svaku grupu inicijalizovati nasumično centar\n",
        "dok se ne dostigne maksimalan broj iteracija ili dok se centri ne prestanu kretati:\n",
        "    pridruži svaki element grupi sa njemu najbližim centrom grupe\n",
        "    ažuriraj pozicije centra svih grupa na osnovu novih elemenata\n",
        "```\n",
        "\n",
        "![img/2/kmeans.gif](img/2/kmeans.gif)\n",
        "\n",
        "Za izračunavanje udaljenosti između podataka u K-means algoritmu koriste se različite metrike sličnosti, od kojih se najčešće koristi Euklidovo rastojanje kao mera bliskosti između tačaka u prostoru.\n",
        "\n",
        "Pošto se početni centri klastera (centroidi) biraju nasumično, algoritam je stohastičke prirode – to znači da rezultati mogu da variraju u zavisnosti od inicijalizacije. Zbog toga se često pokreće više puta, pa se kao konačan rezultat bira onaj sa najboljom (najnižom) vrednošću unutrašnje varijanse ili neke druge funkcije troška.\n",
        "\n",
        "\n",
        "---\n",
        " \n",
        "#### Određivanje optimalnog K\n",
        "\n",
        "Kako znati unapred koliko ima klastera?\n",
        "\n",
        "Kada su podaci dvodimenzionalni, broj klastera se može lako naslutiti putem vizuelizacije. Međutim, u stvarnosti podaci često imaju mnogo više od dve dimenzije, što otežava vizuelnu procenu. U oblasti mašinskog učenja ova pojava je poznata kao \"kletva dimenzionalnosti\" (eng. curse of dimensionality).\n",
        "\n",
        "Određivanje optimalne vrednosti za K (broj klastera) je problem koji je mnogo proučavan. Jedna od najčešće korišćenih heurističkih tehnika za rešavanje ovog problema je tzv. metoda lakta (eng. elbow method).\n",
        "Postupak ide ovako:\n",
        "- Za više različitih vrednosti K (npr. 2, 4, 6, 8, ..., 20) primenjuje se klasterovanje,\n",
        "- Za svaku vrednost K izračunava se suma kvadratnih grešaka (SSE) – ukupna „udaljenost“ svih tačaka od centara svojih klastera,\n",
        "- Matematički, SSE se računa kao:\n",
        "\n",
        "$ SSE = \\sum_{i=1}^{K} \\sum_{x \\in c_{i}} dist(x, c_{i})^{2} $, gde je *dist* euklidska udaljnost.\n",
        "\n",
        "gde je *dist* Euklidska udaljenost, $ c_{i} centar i-tog klastera, a 𝑥 tačka unutar tog klastera.\n",
        "\n",
        "Zatim se nacrta graf na kojem je X osa broj klastera (K), a Y osa vrednost SSE. Na osnovu grafika traži se „lakat“ – tačka nakon koje dalji rast broja klastera dovodi do relativno male promene u SSE. Ta vrednost K se tada uzima kao optimalna jer dalje povećavanje broja klastera više ne donosi značajno poboljšanje kompaktnosti.\n",
        "\n",
        "Primer 1:\n",
        "\n",
        "![img/2/sse2.png](img/2/sse2.png)\n",
        "\n",
        "Primer 2:\n",
        "\n",
        "![img/2/sse.png](img/2/sse.png)\n",
        "\n",
        "---\n",
        "\n",
        "#### Prednosti K-means\n",
        "\n",
        "* Jednostavan i lako razumljiv\n",
        "* Laka implementacija\n",
        "* Relativno dobre performanse (za malo K)\n",
        "* Odličan kada su klasteri sferičnog/globularnog oblika (malo formalnije hiper-sferičnog, za sfere u >3 dimenzija)\n",
        "\n",
        "#### Mane K-means\n",
        "* Potrebno unapred znati K (što je nekad teško odrediti)\n",
        "* Nije deterministički - pošto se centri inicijalizuju nasumično, nekad se dobijaju drugačiji rezultati\n",
        "* Osetljiv na šum\n",
        "* Kada podaci nisu globularnog oblika -> beskoristan (pogledati donju sliku)\n",
        "* Nema mogućnost hijerarhijskog klasterovanja (razlikovanje više manjih podklastera unutar većeg klastera)\n",
        "\n",
        "![img/2/kmeans_fail.png](img/2/kmeans_fail.png)\n",
        "\n",
        "---\n",
        "\n",
        "## DBSCAN (Density-based spatial clustering of applications with noise)\n",
        "\n",
        "DBSCAN je algoritam za klasterovanje podataka koji se zasniva na ideji da klasteri predstavljaju guste regione tačaka u prostoru, dok se retko raspoređene tačke smatraju šumom (engl. noise). Za razliku od algoritama kao što je K-means, DBSCAN ne zahteva da se unapred zada broj klastera, što ga čini pogodnim za podatke nepoznate strukture.\n",
        "\n",
        "Pre nego što objasnimo korake algoritma, moramo objasniti dva pojma: parametre koji određuju kriterijume formiranja klastera i tipove tačaka koje se mogu pojaviti tokom procesa klasterovanja.\n",
        "\n",
        "1. Vrste tačaka:\n",
        "U okviru DBSCAN-a razlikuju se tri vrste tačaka:\n",
        "- Ključne tačke (core points): imaju dovoljno suseda u svojoj blizini (najmanje minPts);\n",
        "- Granične tačke (border points): same nemaju dovoljno suseda, ali se nalaze u epsilon okruženju neke ključne tačke;\n",
        "- Šum (noise): tačke koje ne pripadaju nijednom klasteru.\n",
        "\n",
        "2. Parametri algoritma:\n",
        "DBSCAN koristi dva osnovna parametra:\n",
        "- ε (epsilon): maksimalna udaljenost između dve tačke da bi bile smatrane susedima;\n",
        "- minPts: minimalan broj tačaka potreban da se neka oblast smatra gustim regionom.\n",
        "\n",
        "Udaljenost se najčešće računa pomoću Euklidske distance, ali se mogu koristiti i druge metrike.\n",
        "\n",
        "\n",
        "⚙️ Koraci algoritma:\n",
        "\n",
        "1. Početak algoritma - inicijalizacija početnih tačaka:\n",
        "Izabere se proizvoljna tačka iz skupa. Ako se u njenoj ε (epsilon) okolini nalazi najmanje minPts tačaka, tačka se označava kao ključna i započinje se novi klaster Ako ne ispunjava uslov, privremeno se označava kao šum. Međutim, tačka se kasnije može pridružiti klasteru ukoliko bude obuhvaćena epsilon okolinom neke druge ključne tačke.\n",
        "\n",
        "2. Širenje klastera:\n",
        "Sve tačke u ε (epsilon) okolini ključne tačke dodaju se u klaster. Za svaku od tih tačaka se proverava da li je i sama ključna. Ako jeste, njena okolina se takođe dodaje u klaster (rekurzivno). Proces se nastavlja dok se ne proširi ceo klaster.\n",
        "\n",
        "3. Nastavak algoritma:\n",
        "Nakon završetka jednog klastera, bira se nova neposećena tačka i algoritam se ponavlja. ačka može započeti novi klaster ili biti označena kao šum, u zavisnosti od gustine njene okoline.\n",
        "\n",
        "\n",
        "\n",
        "![img/2/dbscan.png](img/2/dbscan.png)\n",
        "\n",
        "Prikaz rada DBSCAN algoritma:\n",
        "\n",
        "![img/2/DBSCAN_tutorial.gif](img/2/DBSCAN_tutorial.gif)\n",
        "\n",
        "---\n",
        "\n",
        "Razlika između KMeans-a i DBSCAN-a za isti skup podataka:\n",
        "\n",
        "![img/2/kmeans_vs_dbscan.png](img/2/kmeans_vs_dbscan.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_koBAfmewR0H"
      },
      "source": [
        "#### Prednosti DBSCAN\n",
        "\n",
        "* Nije potrebno unapred znati broj klastera (kao kod K-means)\n",
        "* Klasteri mogu biti proizvoljnog oblika\n",
        "* Ume da tretira šum\n",
        "* Parametre epsilon i minPts je lako menjati u cilju dobijanja klastera različitih veličina i oblika, i ove parametre često podešavaju eksperti sa domenskim znanjem\n",
        "\n",
        "\n",
        "#### Mane DBSCAN\n",
        "\n",
        "* Kvalitet rezultata zavisi od toga čime se meri epsilon. Obično je to euklidska udaljenost, ali za višedimenzionalne podatke potrebne su drugačije metrike\n",
        "* Kada postoje varijacije u gustini klastera, nemoguće je odrediti epsilon i minPts da odgovara svim klasterima\n",
        "* U slučaju kada ne postoji ekspert sa domenskim znanjem, određivanje epsilon i minPts parametara je često dosta teško\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "763p-NDIup1r"
      },
      "source": [
        "# Redukcija dimenzionalnosti"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSPNz0IPutIY"
      },
      "source": [
        "\n",
        "### 🌟 Analogija   \n",
        "\n",
        "Gledanje filma u 720p umesto 4K – manje detalja, ali suština ostaje\n",
        "\n",
        "### 📚 Definciija\n",
        "\n",
        "Redukcija dimenzionalnosti je proces smanjenja broja varijabli (osobina) u skupu podataka, pri čemu se pokušava očuvati što više relevantnih informacija. Može se ostvariti metodama selekcije osobina (zadržavanje najvažnijih originalnih osobina) ili ekstrakcije osobina (kreiranje novih, reprezentativnih osobina)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_dZUVxe2OAI"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0QjBN6P2PH4"
      },
      "source": [
        "Principal Component Analysis (PCA) je tehnika koja se koristi za redukciju dimenzionalnosti podataka. Njena osnovna ideja je da pronađe nove osobine podataka (tzv. glavne komponente) koje sadrže što više informacije iz originalnih osobina, ali sa smanjenom dimenzionalnošću.\n",
        "\n",
        "Zašto je korisno?\n",
        "1. Ubrzava algoritme i smanjuje memorijske resurse\n",
        "2. Olakšava vizuelizaciju (npr. 3D → 2D)\n",
        "3. Smanjuje šum\n",
        "\n",
        "![img/2/pca.gif](img/2/pca.gif)\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ Kako funkcioniše PCA?\n",
        "\n",
        "PCA funkcioniše tako što projektuje podatke u novu koordinatnu ravan, pri čemu se bira pravac najveće varijanse podataka. To se radi kroz sledeće korake:\n",
        "\n",
        "1. Standardizacija podataka – podatke treba normalizovati kako bi svi atributi imali isti značaj (npr. skalirati ih da imaju srednju vrednost 0 i standardnu devijaciju 1).\n",
        "2. Proračun kovarijacione matrice – određuje se kako su varijable međusobno povezane.\n",
        "3. Proračun sopstvenih vrednosti i sopstvenih vektora – sopstveni vektori predstavljaju pravce glavnih komponenti, dok sopstvene vrednosti određuju značaj tih pravaca.\n",
        "4. Odabir glavnih komponenti – bira se određen broj komponenti koje objašnjavaju najveći deo varijanse podataka.\n",
        "\n",
        "Transformacija podataka – originalni podaci se projektuju na nove glavne komponente.\n",
        "\n",
        "---\n",
        "\n",
        "##### Prednosti PCA\n",
        "\n",
        "* Smanjuje dimenzionalnost podataka, čime se poboljšavaju performanse\n",
        "algoritama i smanjuje problem „kletve dimenzionalnosti”.\n",
        "* Čuva što je više moguće informacije iz originalnih podataka uz minimalan gubitak.\n",
        "* Ubrzava treniranje modela jer smanjuje broj osobina.\n",
        "* Koristi se za vizualizaciju podataka (PCA može projicirati visoko-dimenzionalne podatke u 2D ili 3D prostor za lakšu interpretaciju).\n",
        "* Otkriva latentne strukture u podacima, omogućavajući bolju interpretaciju.\n",
        "\n",
        "##### Mane PCA\n",
        "\n",
        "* Može izgubiti važne informacije, posebno ako se izabere premali broj komponenti.\n",
        "* Nije idealan za nelinearne odnose – PCA bazira transformaciju na linearnim kombinacijama osobina.\n",
        "* Osetljiv na skaliranje podataka – loše skalirani podaci mogu dovesti do loših rezultata.\n",
        "* Teže interpretirati nove glavne komponente – one često nemaju očigledno značenje u odnosu na originalne osobine.\n",
        "  \n",
        "![img/2/pca.png](img/2/pca.png)\n",
        "\n",
        "PCA (3D → 2D):\n",
        " - Prva komponenta objašnjava 62.48% varijanse\n",
        " - Druga komponenta objašnjava 25.56% varijanse\n",
        "\n",
        "PCA (3D → 1D):\n",
        " - Prva komponenta objašnjava 62.48% varijanse"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
