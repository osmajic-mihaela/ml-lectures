{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X06B2GHlfgUd"
      },
      "source": [
        "# Nenadgledano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2AlpLvuvj_w"
      },
      "source": [
        "U nenadgledanom uÄenju model se trenira na osnovu x vrednosti primera, bez prateÄ‡ih izlaza y. Cilj je da se otkrije â€skrivenaâ€œ struktura u podacima, odnosno da se nauÄi reprezentacija koja opisuje unutraÅ¡nje odnose meÄ‘u podacima. Ovakav pristup moÅ¾e pomoÄ‡i u identifikaciji obrazaca i veza u okviru skupa podataka. PoÅ¡to nema poznatih izlaza, ne postoji jedinstven standard za merenje performansi modela. \n",
        "Postupci se dele na:\n",
        "* Klasterizacija/klasterovanje  - grupisanje podataka na osnovu sliÄnosti\n",
        "* Redukcija dimenzionalnosti  - otkrivanje najdeskriptivnijih osobina podataka\n",
        "* Asocijativna pravila  - pronalaÅ¾enje uobiÄajenih kombinacija osobina podataka\n",
        "\n",
        "\n",
        "![img/2/unsupervised-lr.jpg](img/2/unsupervised-lr.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bx59rWEvT6u"
      },
      "source": [
        "# Klasterovanje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44e-isyrvWss"
      },
      "source": [
        "### ğŸ“š Definicija\n",
        "\n",
        "Klasterovanje je tip nenadgledanog uÄenja koji se koristi za grupisanje meÄ‘usobno sliÄnih podataka.\n",
        "\n",
        "ÄŒesto se koristi za redukciju dimenzionalnosti i detekciju autlajera.\n",
        "\n",
        "---\n",
        "\n",
        "## K Means\n",
        "\n",
        "K-means spada u ne-hijerarhijske metode grupisanja, Å¡to znaÄi da unapred definiÅ¡e broj grupa (klastera) i pokuÅ¡ava da dodeli podatke tim grupama na osnovu njihove meÄ‘usobne sliÄnosti. Ovaj algoritam se Äesto koristi u okviru procesa eksplorativne analize podataka, kada Å¾elimo da bolje razumemo strukturu skupa podataka, bez prethodnog znanja o tome kako bi oni trebalo da budu rasporeÄ‘eni.\n",
        "\n",
        "âš™ï¸ Pseudo-kod:\n",
        "\n",
        "```\n",
        "za svaku grupu inicijalizovati nasumiÄno centar\n",
        "dok se ne dostigne maksimalan broj iteracija ili dok se centri ne prestanu kretati:\n",
        "    pridruÅ¾i svaki element grupi sa njemu najbliÅ¾im centrom grupe\n",
        "    aÅ¾uriraj pozicije centra svih grupa na osnovu novih elemenata\n",
        "```\n",
        "\n",
        "![img/2/kmeans.gif](img/2/kmeans.gif)\n",
        "\n",
        "Za izraÄunavanje udaljenosti izmeÄ‘u podataka u K-means algoritmu koriste se razliÄite metrike sliÄnosti, od kojih se najÄeÅ¡Ä‡e koristi Euklidovo rastojanje kao mera bliskosti izmeÄ‘u taÄaka u prostoru.\n",
        "\n",
        "PoÅ¡to se poÄetni centri klastera (centroidi) biraju nasumiÄno, algoritam je stohastiÄke prirode â€“ to znaÄi da rezultati mogu da variraju u zavisnosti od inicijalizacije. Zbog toga se Äesto pokreÄ‡e viÅ¡e puta, pa se kao konaÄan rezultat bira onaj sa najboljom (najniÅ¾om) vrednoÅ¡Ä‡u unutraÅ¡nje varijanse ili neke druge funkcije troÅ¡ka.\n",
        "\n",
        "\n",
        "---\n",
        " \n",
        "#### OdreÄ‘ivanje optimalnog K\n",
        "\n",
        "Kako znati unapred koliko ima klastera?\n",
        "\n",
        "Kada su podaci dvodimenzionalni, broj klastera se moÅ¾e lako naslutiti putem vizuelizacije. MeÄ‘utim, u stvarnosti podaci Äesto imaju mnogo viÅ¡e od dve dimenzije, Å¡to oteÅ¾ava vizuelnu procenu. U oblasti maÅ¡inskog uÄenja ova pojava je poznata kao \"kletva dimenzionalnosti\" (eng. curse of dimensionality).\n",
        "\n",
        "OdreÄ‘ivanje optimalne vrednosti za K (broj klastera) je problem koji je mnogo prouÄavan. Jedna od najÄeÅ¡Ä‡e koriÅ¡Ä‡enih heuristiÄkih tehnika za reÅ¡avanje ovog problema je tzv. metoda lakta (eng. elbow method).\n",
        "Postupak ide ovako:\n",
        "- Za viÅ¡e razliÄitih vrednosti K (npr. 2, 4, 6, 8, ..., 20) primenjuje se klasterovanje,\n",
        "- Za svaku vrednost K izraÄunava se suma kvadratnih greÅ¡aka (SSE) â€“ ukupna â€udaljenostâ€œ svih taÄaka od centara svojih klastera,\n",
        "- MatematiÄki, SSE se raÄuna kao:\n",
        "\n",
        "$ SSE = \\sum_{i=1}^{K} \\sum_{x \\in c_{i}} dist(x, c_{i})^{2} $, gde je *dist* euklidska udaljnost.\n",
        "\n",
        "gde je *dist* Euklidska udaljenost, $ c_{i} centar i-tog klastera, a ğ‘¥ taÄka unutar tog klastera.\n",
        "\n",
        "Zatim se nacrta graf na kojem je X osa broj klastera (K), a Y osa vrednost SSE. Na osnovu grafika traÅ¾i se â€lakatâ€œ â€“ taÄka nakon koje dalji rast broja klastera dovodi do relativno male promene u SSE. Ta vrednost K se tada uzima kao optimalna jer dalje poveÄ‡avanje broja klastera viÅ¡e ne donosi znaÄajno poboljÅ¡anje kompaktnosti.\n",
        "\n",
        "Primer 1:\n",
        "\n",
        "![img/2/sse2.png](img/2/sse2.png)\n",
        "\n",
        "Primer 2:\n",
        "\n",
        "![img/2/sse.png](img/2/sse.png)\n",
        "\n",
        "---\n",
        "\n",
        "#### Prednosti K-means\n",
        "\n",
        "* Jednostavan i lako razumljiv\n",
        "* Laka implementacija\n",
        "* Relativno dobre performanse (za malo K)\n",
        "* OdliÄan kada su klasteri sferiÄnog/globularnog oblika (malo formalnije hiper-sferiÄnog, za sfere u >3 dimenzija)\n",
        "\n",
        "#### Mane K-means\n",
        "* Potrebno unapred znati K (Å¡to je nekad teÅ¡ko odrediti)\n",
        "* Nije deterministiÄki - poÅ¡to se centri inicijalizuju nasumiÄno, nekad se dobijaju drugaÄiji rezultati\n",
        "* Osetljiv na Å¡um\n",
        "* Kada podaci nisu globularnog oblika -> beskoristan (pogledati donju sliku)\n",
        "* Nema moguÄ‡nost hijerarhijskog klasterovanja (razlikovanje viÅ¡e manjih podklastera unutar veÄ‡eg klastera)\n",
        "\n",
        "![img/2/kmeans_fail.png](img/2/kmeans_fail.png)\n",
        "\n",
        "---\n",
        "\n",
        "## DBSCAN (Density-based spatial clustering of applications with noise)\n",
        "\n",
        "DBSCAN je algoritam za klasterovanje podataka koji se zasniva na ideji da klasteri predstavljaju guste regione taÄaka u prostoru, dok se retko rasporeÄ‘ene taÄke smatraju Å¡umom (engl. noise). Za razliku od algoritama kao Å¡to je K-means, DBSCAN ne zahteva da se unapred zada broj klastera, Å¡to ga Äini pogodnim za podatke nepoznate strukture.\n",
        "\n",
        "Pre nego Å¡to objasnimo korake algoritma, moramo objasniti dva pojma: parametre koji odreÄ‘uju kriterijume formiranja klastera i tipove taÄaka koje se mogu pojaviti tokom procesa klasterovanja.\n",
        "\n",
        "1. Vrste taÄaka:\n",
        "U okviru DBSCAN-a razlikuju se tri vrste taÄaka:\n",
        "- KljuÄne taÄke (core points): imaju dovoljno suseda u svojoj blizini (najmanje minPts);\n",
        "- GraniÄne taÄke (border points): same nemaju dovoljno suseda, ali se nalaze u epsilon okruÅ¾enju neke kljuÄne taÄke;\n",
        "- Å um (noise): taÄke koje ne pripadaju nijednom klasteru.\n",
        "\n",
        "2. Parametri algoritma:\n",
        "DBSCAN koristi dva osnovna parametra:\n",
        "- Îµ (epsilon): maksimalna udaljenost izmeÄ‘u dve taÄke da bi bile smatrane susedima;\n",
        "- minPts: minimalan broj taÄaka potreban da se neka oblast smatra gustim regionom.\n",
        "\n",
        "Udaljenost se najÄeÅ¡Ä‡e raÄuna pomoÄ‡u Euklidske distance, ali se mogu koristiti i druge metrike.\n",
        "\n",
        "\n",
        "âš™ï¸ Koraci algoritma:\n",
        "\n",
        "1. PoÄetak algoritma - inicijalizacija poÄetnih taÄaka:\n",
        "Izabere se proizvoljna taÄka iz skupa. Ako se u njenoj Îµ (epsilon) okolini nalazi najmanje minPts taÄaka, taÄka se oznaÄava kao kljuÄna i zapoÄinje se novi klaster Ako ne ispunjava uslov, privremeno se oznaÄava kao Å¡um. MeÄ‘utim, taÄka se kasnije moÅ¾e pridruÅ¾iti klasteru ukoliko bude obuhvaÄ‡ena epsilon okolinom neke druge kljuÄne taÄke.\n",
        "\n",
        "2. Å irenje klastera:\n",
        "Sve taÄke u Îµ (epsilon) okolini kljuÄne taÄke dodaju se u klaster. Za svaku od tih taÄaka se proverava da li je i sama kljuÄna. Ako jeste, njena okolina se takoÄ‘e dodaje u klaster (rekurzivno). Proces se nastavlja dok se ne proÅ¡iri ceo klaster.\n",
        "\n",
        "3. Nastavak algoritma:\n",
        "Nakon zavrÅ¡etka jednog klastera, bira se nova neposeÄ‡ena taÄka i algoritam se ponavlja. aÄka moÅ¾e zapoÄeti novi klaster ili biti oznaÄena kao Å¡um, u zavisnosti od gustine njene okoline.\n",
        "\n",
        "\n",
        "\n",
        "![img/2/dbscan.png](img/2/dbscan.png)\n",
        "\n",
        "Prikaz rada DBSCAN algoritma:\n",
        "\n",
        "![img/2/DBSCAN_tutorial.gif](img/2/DBSCAN_tutorial.gif)\n",
        "\n",
        "---\n",
        "\n",
        "Razlika izmeÄ‘u KMeans-a i DBSCAN-a za isti skup podataka:\n",
        "\n",
        "![img/2/kmeans_vs_dbscan.png](img/2/kmeans_vs_dbscan.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_koBAfmewR0H"
      },
      "source": [
        "#### Prednosti DBSCAN\n",
        "\n",
        "* Nije potrebno unapred znati broj klastera (kao kod K-means)\n",
        "* Klasteri mogu biti proizvoljnog oblika\n",
        "* Ume da tretira Å¡um\n",
        "* Parametre epsilon i minPts je lako menjati u cilju dobijanja klastera razliÄitih veliÄina i oblika, i ove parametre Äesto podeÅ¡avaju eksperti sa domenskim znanjem\n",
        "\n",
        "\n",
        "#### Mane DBSCAN\n",
        "\n",
        "* Kvalitet rezultata zavisi od toga Äime se meri epsilon. ObiÄno je to euklidska udaljenost, ali za viÅ¡edimenzionalne podatke potrebne su drugaÄije metrike\n",
        "* Kada postoje varijacije u gustini klastera, nemoguÄ‡e je odrediti epsilon i minPts da odgovara svim klasterima\n",
        "* U sluÄaju kada ne postoji ekspert sa domenskim znanjem, odreÄ‘ivanje epsilon i minPts parametara je Äesto dosta teÅ¡ko\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "763p-NDIup1r"
      },
      "source": [
        "# Redukcija dimenzionalnosti"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSPNz0IPutIY"
      },
      "source": [
        "\n",
        "### ğŸŒŸ Analogija   \n",
        "\n",
        "Gledanje filma u 720p umesto 4K â€“ manje detalja, ali suÅ¡tina ostaje\n",
        "\n",
        "### ğŸ“š Definciija\n",
        "\n",
        "Redukcija dimenzionalnosti je proces smanjenja broja varijabli (osobina) u skupu podataka, pri Äemu se pokuÅ¡ava oÄuvati Å¡to viÅ¡e relevantnih informacija. MoÅ¾e se ostvariti metodama selekcije osobina (zadrÅ¾avanje najvaÅ¾nijih originalnih osobina) ili ekstrakcije osobina (kreiranje novih, reprezentativnih osobina)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_dZUVxe2OAI"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0QjBN6P2PH4"
      },
      "source": [
        "Principal Component Analysis (PCA) je tehnika koja se koristi za redukciju dimenzionalnosti podataka. Njena osnovna ideja je da pronaÄ‘e nove osobine podataka (tzv. glavne komponente) koje sadrÅ¾e Å¡to viÅ¡e informacije iz originalnih osobina, ali sa smanjenom dimenzionalnoÅ¡Ä‡u.\n",
        "\n",
        "ZaÅ¡to je korisno?\n",
        "1. Ubrzava algoritme i smanjuje memorijske resurse\n",
        "2. OlakÅ¡ava vizuelizaciju (npr. 3D â†’ 2D)\n",
        "3. Smanjuje Å¡um\n",
        "\n",
        "![img/2/pca.gif](img/2/pca.gif)\n",
        "\n",
        "---\n",
        "\n",
        "### âš™ï¸ Kako funkcioniÅ¡e PCA?\n",
        "\n",
        "PCA funkcioniÅ¡e tako Å¡to projektuje podatke u novu koordinatnu ravan, pri Äemu se bira pravac najveÄ‡e varijanse podataka. To se radi kroz sledeÄ‡e korake:\n",
        "\n",
        "1. Standardizacija podataka â€“ podatke treba normalizovati kako bi svi atributi imali isti znaÄaj (npr. skalirati ih da imaju srednju vrednost 0 i standardnu devijaciju 1).\n",
        "2. ProraÄun kovarijacione matrice â€“ odreÄ‘uje se kako su varijable meÄ‘usobno povezane.\n",
        "3. ProraÄun sopstvenih vrednosti i sopstvenih vektora â€“ sopstveni vektori predstavljaju pravce glavnih komponenti, dok sopstvene vrednosti odreÄ‘uju znaÄaj tih pravaca.\n",
        "4. Odabir glavnih komponenti â€“ bira se odreÄ‘en broj komponenti koje objaÅ¡njavaju najveÄ‡i deo varijanse podataka.\n",
        "\n",
        "Transformacija podataka â€“ originalni podaci se projektuju na nove glavne komponente.\n",
        "\n",
        "---\n",
        "\n",
        "##### Prednosti PCA\n",
        "\n",
        "* Smanjuje dimenzionalnost podataka, Äime se poboljÅ¡avaju performanse\n",
        "algoritama i smanjuje problem â€kletve dimenzionalnostiâ€.\n",
        "* ÄŒuva Å¡to je viÅ¡e moguÄ‡e informacije iz originalnih podataka uz minimalan gubitak.\n",
        "* Ubrzava treniranje modela jer smanjuje broj osobina.\n",
        "* Koristi se za vizualizaciju podataka (PCA moÅ¾e projicirati visoko-dimenzionalne podatke u 2D ili 3D prostor za lakÅ¡u interpretaciju).\n",
        "* Otkriva latentne strukture u podacima, omoguÄ‡avajuÄ‡i bolju interpretaciju.\n",
        "\n",
        "##### Mane PCA\n",
        "\n",
        "* MoÅ¾e izgubiti vaÅ¾ne informacije, posebno ako se izabere premali broj komponenti.\n",
        "* Nije idealan za nelinearne odnose â€“ PCA bazira transformaciju na linearnim kombinacijama osobina.\n",
        "* Osetljiv na skaliranje podataka â€“ loÅ¡e skalirani podaci mogu dovesti do loÅ¡ih rezultata.\n",
        "* TeÅ¾e interpretirati nove glavne komponente â€“ one Äesto nemaju oÄigledno znaÄenje u odnosu na originalne osobine.\n",
        "  \n",
        "![img/2/pca.png](img/2/pca.png)\n",
        "\n",
        "PCA (3D â†’ 2D):\n",
        " - Prva komponenta objaÅ¡njava 62.48% varijanse\n",
        " - Druga komponenta objaÅ¡njava 25.56% varijanse\n",
        "\n",
        "PCA (3D â†’ 1D):\n",
        " - Prva komponenta objaÅ¡njava 62.48% varijanse"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
