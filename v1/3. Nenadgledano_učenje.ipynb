{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X06B2GHlfgUd"
      },
      "source": [
        "# Nenadgledano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2AlpLvuvj_w"
      },
      "source": [
        "Učenje funkcije koja opisuje \"skrivenu\" strukturu neobeleženih podataka, odnosno učenje reprezentacije podataka na osnovu primera *x* (nema *y*). Nenadgledano učenje može biti korisno za pronalaženje interesantnih veza među podacima. Ne postoji neki standardni način merenja performansi. Postupci se dele na:\n",
        "* Klasterizacija/klasterovanje  - grupisanje podataka na osnovu sličnosti\n",
        "* Redukcija dimenzionalnosti  - otkrivanje najdeskriptivnijih osobina podataka\n",
        "* Asocijativna pravila  - pronalaženje uobičajenih kombinacija osobina podataka\n",
        "\n",
        "\n",
        "![img/2/unsupervised-lr.jpg](img/2/unsupervised-lr.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bx59rWEvT6u"
      },
      "source": [
        "# Klasterovanje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44e-isyrvWss"
      },
      "source": [
        "### 📚 Definicija\n",
        "\n",
        "Klasterovanje je tip nenadgledanog učenja koji se koristi za grupisanje međusobno sličnih podataka.\n",
        "\n",
        "Često se koristi za redukciju dimenzionalnosti i detekciju autlajera.\n",
        "\n",
        "---\n",
        "\n",
        "## K Means\n",
        "\n",
        "Jedan od najčešće korišćenih algoritama za nenadgledano klasterovanje podataka. Preciznije, k-means je ne-hijerarhijska metoda grupisanja sličnih podataka. K-means je tehnika koja se često koristi u tzv. *eksplorativnoj analizi podataka*.\n",
        "\n",
        "Klasterizacija je zadatak grupisanja skupa objekata, tako da su objekti koji su u istoj grupi (odnosno *klasteru*) sličniji (u nekom smislu) jedni drugima, više nego što su slični objektima u drugim grupama (klasterima).\n",
        "\n",
        "⚙️ Pseudo-kod:\n",
        "\n",
        "```\n",
        "za svaku grupu inicijalizovati nasumično centar\n",
        "dok se centri ne prestanu kretati ili ne dostigne max broj iteracija:\n",
        "    pridruži svaki element grupi sa njemu najbližim centrom grupe\n",
        "    pomeri centar svih grupa na osnovu novih elemenata\n",
        "```\n",
        "\n",
        "![img/2/kmeans.gif](img/2/kmeans.gif)\n",
        "\n",
        "Za računanje distance se najčešće koristi euklidsko rastojanje.\n",
        "\n",
        "Kako se centri inicijalizuju nasumično, K-means je stohastički algoritam.\n",
        "\n",
        "---\n",
        " \n",
        "#### Određivanje optimalnog K\n",
        "\n",
        "Kako znati unapred koliko ima klastera?\n",
        "\n",
        "Lako je videti kada su podaci dvodimenzionalni, jer ih je onda lako i vizualizovati, ali često podaci imaju (mnogo) više od samo 2 dimenzije - ovo je tzv. \"kletva dimenzionalnosti\" (*eng. curse of dimensionality*) u mašinskom učenju.\n",
        "\n",
        "Određivanje optimalnog K (tj. broja klastera) je nešto se dosta proučavalo, a mi ćemo koristi tzv. \"metodu lakta\" (*eng. elbow method*). Za određen broj K (npr. 2, 4, 6, 8, ..., 20) se vrši klasterizacija i zatim se računa suma kvadratnih grešaka (SSE). SSE se računa tako što se unutar svakog klastera sumiraju kvadrati udaljenosti podataka od centra klastera, i zatim se sve to opet sumira. Matematički:\n",
        "\n",
        "$ SSE = \\sum_{i=1}^{K} \\sum_{x \\in c_{i}} dist(x, c_{i})^{2} $, gde je *dist* euklidska udaljnost.\n",
        "\n",
        "Zatim se za sve plotuje SSE u odnosu na K, npr.:\n",
        "\n",
        "![img/2/sse.png](img/2/sse.png)\n",
        "\n",
        "---\n",
        "\n",
        "#### Prednosti K-means\n",
        "\n",
        "* Jednostavan i lako razumljiv\n",
        "* Laka implementacija\n",
        "* Relativno dobre performanse (za malo K)\n",
        "* Odličan kada su klasteri sferičnog/globularnog oblika (malo formalnije hiper-sferičnog, za sfere u >3 dimenzija)\n",
        "\n",
        "#### Mane K-means\n",
        "* Potrebno unapred znati K (što je nekad teško odrediti)\n",
        "* Nije deterministički - pošto se centri inicijalizuju nasumično, nekad se dobijaju drugačiji rezultati\n",
        "* Osetljiv na šum\n",
        "* Kada podaci nisu globularnog oblika -> beskoristan (pogledati donju sliku)\n",
        "* Nema mogućnost hijerarhijskog klasterovanja (razlikovanje više manjih podklastera unutar većeg klastera)\n",
        "\n",
        "![img/2/kmeans_fail.png](img/2/kmeans_fail.png)\n",
        "\n",
        "---\n",
        "\n",
        "## DBSCAN\n",
        "\n",
        "DBSCAN (Density-based spatial clustering of applications with noise) je takođe algoritam za klasterizaciju podataka. Ovaj postupak se zasniva na ideji grupisanja tačaka (podataka) na osnovu njihove međusobne udaljenosti. Ukoliko se tačke nalaze u tzv. *epsilon okolini* one su deo nekog klastera, u suprotnom se posmatraju kao šum.\n",
        "\n",
        "⚙️ Opis DBSCAN algoritma:\n",
        "1. Neka postoji neki skup tačaka (podataka) koje želimo da klasterizujemo. U samom postupku, razlikuju se tri vrste tačaka: ključne tačke, dostupne tačke i šum.\n",
        "2. DBSCAN zahteva dva parametra: *epsilon* (eps) i *minimalni broj potrebnih tačaka koje čine region* (minPts). Epsilon okolina se najčešće računa korišćenjem euklidske udaljenosti.\n",
        "3. Algoritam počinje sa proizvoljnom tačkom. Računa se epsilon okolina te tačke, i ukoliko se u njoj nalazi dovoljno tačaka (minPts), započinje se novi klaster. U suprotnom, tačka se računa kao šum. Obratiti pažnju da tačka, iako je šum, kasnije *može* biti pronađena kao deo neke druge epsilon okoline sa dovoljno tačaka i samim tim da postane deo klastera.\n",
        "4. Ukoliko je za tačku određeno da pripada klasteru, sve tačke u njenoj epsilon okolini takođe pripadaju tom klasteru. Dakle, sve tačke koje su pronađene u epsilon okolini trenutne tačke se dodaju u klaster, kao i tačke koje se nalaze u epsilon okolini tih tačaka (rekurzivno). Proces se nastavlja dok se ne nađe ceo klaster, odnosno dok se ne obiđu sve tačke u epsilon okolinama.\n",
        "5. Onda se nalazi nova, prozivoljna neposećena tačka, za koju se ponavlja čitav postupak, što dovodi do otkrivanja ili novog klastera ili šuma.\n",
        "\n",
        "![img/2/dbscan.png](img/2/dbscan.png)\n",
        "\n",
        "---\n",
        "Obratite pažnju na prikaz sledećeg skupa podataka:\n",
        "\n",
        "![img/2/data.png](img/2/data.png)\n",
        "\n",
        "Hajde da vidimo rezultat DB scan-a naspram KMeans za isti skup:\n",
        "\n",
        "![img/2/kmeans_vs_dbscan.png](img/2/kmeans_vs_dbscan.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_koBAfmewR0H"
      },
      "source": [
        "#### Prednosti DBSCAN\n",
        "\n",
        "* Nije potrebno unapred znati broj klastera (kao kod K-means)\n",
        "* Klasteri mogu biti proizvoljnog oblika\n",
        "* Ume da tretira šum\n",
        "* Parametre epsilon i minPts je lako menjati u cilju dobijanja klastera različitih veličina i oblika, i ove parametre često podešavaju eksperti sa domenskim znanjem\n",
        "\n",
        "\n",
        "#### Mane DBSCAN\n",
        "\n",
        "* Kvalitet rezultata zavisi od toga čime se meri epsilon. Obično je to euklidska udaljenost, ali za višedimenzionalne podatke potrebne su drugačije metrike\n",
        "* Kada postoje varijacije u gustini klastera, nemoguće je odrediti epsilon i minPts da odgovara svim klasterima\n",
        "* U slučaju kada ne postoji ekspert sa domenskim znanjem, određivanje epsilon i minPts parametara je često dosta teško\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "763p-NDIup1r"
      },
      "source": [
        "# Redukcija dimenzionalnosti"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSPNz0IPutIY"
      },
      "source": [
        "\n",
        "### 🌟 Analogija   \n",
        "\n",
        "Gledanje filma u 720p umesto 4K – manje detalja, ali suština ostaje\n",
        "\n",
        "### 📚 Definciija\n",
        "\n",
        "Redukcija dimenzionalnosti je proces smanjenja broja varijabli (osobina) u skupu podataka, pri čemu se pokušava očuvati što više relevantnih informacija. Može se ostvariti metodama selekcije osobina (zadržavanje najvažnijih originalnih osobina) ili ekstrakcije osobina (kreiranje novih, reprezentativnih osobina)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_dZUVxe2OAI"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0QjBN6P2PH4"
      },
      "source": [
        "Principal Component Analysis (PCA) je tehnika koja se koristi za redukciju dimenzionalnosti podataka. Njena osnovna ideja je da pronađe nove osobine podataka (tzv. glavne komponente) koje sadrže što više informacije iz originalnih osobina, ali sa smanjenom dimenzionalnošću.\n",
        "\n",
        "Zašto je korisno?\n",
        "1. Ubrzava algoritme i smanjuje memorijske resurse\n",
        "2. Olakšava vizuelizaciju (npr. 3D → 2D)\n",
        "3. Smanjuje šum\n",
        "\n",
        "![img/2/pca.gif](img/2/pca.gif)\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ Kako funkcioniše PCA?\n",
        "\n",
        "PCA funkcioniše tako što projektuje podatke u novu koordinatnu ravan, pri čemu se bira pravac najveće varijanse podataka. To se radi kroz sledeće korake:\n",
        "\n",
        "1. Standardizacija podataka – podatke treba normalizovati kako bi svi atributi imali isti značaj (npr. skalirati ih da imaju srednju vrednost 0 i standardnu devijaciju 1).\n",
        "2. Proračun kovarijacione matrice – određuje se kako su varijable međusobno povezane.\n",
        "3. Proračun sopstvenih vrednosti i sopstvenih vektora – sopstveni vektori predstavljaju pravce glavnih komponenti, dok sopstvene vrednosti određuju značaj tih pravaca.\n",
        "4. Odabir glavnih komponenti – bira se određen broj komponenti koje objašnjavaju najveći deo varijanse podataka.\n",
        "\n",
        "Transformacija podataka – originalni podaci se projektuju na nove glavne komponente.\n",
        "\n",
        "---\n",
        "\n",
        "##### Prednosti PCA\n",
        "\n",
        "* Smanjuje dimenzionalnost podataka, čime se poboljšavaju performanse\n",
        "algoritama i smanjuje problem „kletve dimenzionalnosti”.\n",
        "* Čuva što je više moguće informacije iz originalnih podataka uz minimalan gubitak.\n",
        "* Ubrzava treniranje modela jer smanjuje broj osobina.\n",
        "* Koristi se za vizualizaciju podataka (PCA može projicirati visoko-dimenzionalne podatke u 2D ili 3D prostor za lakšu interpretaciju).\n",
        "* Otkriva latentne strukture u podacima, omogućavajući bolju interpretaciju.\n",
        "\n",
        "##### Mane PCA\n",
        "\n",
        "* Može izgubiti važne informacije, posebno ako se izabere premali broj komponenti.\n",
        "* Nije idealan za nelinearne odnose – PCA bazira transformaciju na linearnim kombinacijama osobina.\n",
        "* Osetljiv na skaliranje podataka – loše skalirani podaci mogu dovesti do loših rezultata.\n",
        "* Teže interpretirati nove glavne komponente – one često nemaju očigledno značenje u odnosu na originalne osobine.\n",
        "  \n",
        "![img/2/pca.png](img/2/pca.png)\n",
        "\n",
        "PCA (3D → 2D):\n",
        " - Prva komponenta objašnjava 62.48% varijanse\n",
        " - Druga komponenta objašnjava 25.56% varijanse\n",
        "\n",
        "PCA (3D → 1D):\n",
        " - Prva komponenta objašnjava 62.48% varijanse"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
