{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X06B2GHlfgUd"
      },
      "source": [
        "# Nenadgledano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2AlpLvuvj_w"
      },
      "source": [
        "UÄenje funkcije koja opisuje \"skrivenu\" strukturu neobeleÅ¾enih podataka, odnosno uÄenje reprezentacije podataka na osnovu primera *x* (nema *y*). Nenadgledano uÄenje moÅ¾e biti korisno za pronalaÅ¾enje interesantnih veza meÄ‘u podacima. Ne postoji neki standardni naÄin merenja performansi. Postupci se dele na:\n",
        "* Klasterizacija/klasterovanje  - grupisanje podataka na osnovu sliÄnosti\n",
        "* Redukcija dimenzionalnosti  - otkrivanje najdeskriptivnijih osobina podataka\n",
        "* Asocijativna pravila  - pronalaÅ¾enje uobiÄajenih kombinacija osobina podataka\n",
        "\n",
        "\n",
        "![img/2/unsupervised-lr.jpg](img/2/unsupervised-lr.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bx59rWEvT6u"
      },
      "source": [
        "# Klasterovanje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44e-isyrvWss"
      },
      "source": [
        "### ğŸ“š Definicija\n",
        "\n",
        "Klasterovanje je tip nenadgledanog uÄenja koji se koristi za grupisanje meÄ‘usobno sliÄnih podataka.\n",
        "\n",
        "ÄŒesto se koristi za redukciju dimenzionalnosti i detekciju autlajera.\n",
        "\n",
        "---\n",
        "\n",
        "## K Means\n",
        "\n",
        "Jedan od najÄeÅ¡Ä‡e koriÅ¡Ä‡enih algoritama za nenadgledano klasterovanje podataka. Preciznije, k-means je ne-hijerarhijska metoda grupisanja sliÄnih podataka. K-means je tehnika koja se Äesto koristi u tzv. *eksplorativnoj analizi podataka*.\n",
        "\n",
        "Klasterizacija je zadatak grupisanja skupa objekata, tako da su objekti koji su u istoj grupi (odnosno *klasteru*) sliÄniji (u nekom smislu) jedni drugima, viÅ¡e nego Å¡to su sliÄni objektima u drugim grupama (klasterima).\n",
        "\n",
        "âš™ï¸ Pseudo-kod:\n",
        "\n",
        "```\n",
        "za svaku grupu inicijalizovati nasumiÄno centar\n",
        "dok se centri ne prestanu kretati ili ne dostigne max broj iteracija:\n",
        "    pridruÅ¾i svaki element grupi sa njemu najbliÅ¾im centrom grupe\n",
        "    pomeri centar svih grupa na osnovu novih elemenata\n",
        "```\n",
        "\n",
        "![img/2/kmeans.gif](img/2/kmeans.gif)\n",
        "\n",
        "Za raÄunanje distance se najÄeÅ¡Ä‡e koristi euklidsko rastojanje.\n",
        "\n",
        "Kako se centri inicijalizuju nasumiÄno, K-means je stohastiÄki algoritam.\n",
        "\n",
        "---\n",
        " \n",
        "#### OdreÄ‘ivanje optimalnog K\n",
        "\n",
        "Kako znati unapred koliko ima klastera?\n",
        "\n",
        "Lako je videti kada su podaci dvodimenzionalni, jer ih je onda lako i vizualizovati, ali Äesto podaci imaju (mnogo) viÅ¡e od samo 2 dimenzije - ovo je tzv. \"kletva dimenzionalnosti\" (*eng. curse of dimensionality*) u maÅ¡inskom uÄenju.\n",
        "\n",
        "OdreÄ‘ivanje optimalnog K (tj. broja klastera) je neÅ¡to se dosta prouÄavalo, a mi Ä‡emo koristi tzv. \"metodu lakta\" (*eng. elbow method*). Za odreÄ‘en broj K (npr. 2, 4, 6, 8, ..., 20) se vrÅ¡i klasterizacija i zatim se raÄuna suma kvadratnih greÅ¡aka (SSE). SSE se raÄuna tako Å¡to se unutar svakog klastera sumiraju kvadrati udaljenosti podataka od centra klastera, i zatim se sve to opet sumira. MatematiÄki:\n",
        "\n",
        "$ SSE = \\sum_{i=1}^{K} \\sum_{x \\in c_{i}} dist(x, c_{i})^{2} $, gde je *dist* euklidska udaljnost.\n",
        "\n",
        "Zatim se za sve plotuje SSE u odnosu na K, npr.:\n",
        "\n",
        "![img/2/sse.png](img/2/sse.png)\n",
        "\n",
        "---\n",
        "\n",
        "#### Prednosti K-means\n",
        "\n",
        "* Jednostavan i lako razumljiv\n",
        "* Laka implementacija\n",
        "* Relativno dobre performanse (za malo K)\n",
        "* OdliÄan kada su klasteri sferiÄnog/globularnog oblika (malo formalnije hiper-sferiÄnog, za sfere u >3 dimenzija)\n",
        "\n",
        "#### Mane K-means\n",
        "* Potrebno unapred znati K (Å¡to je nekad teÅ¡ko odrediti)\n",
        "* Nije deterministiÄki - poÅ¡to se centri inicijalizuju nasumiÄno, nekad se dobijaju drugaÄiji rezultati\n",
        "* Osetljiv na Å¡um\n",
        "* Kada podaci nisu globularnog oblika -> beskoristan (pogledati donju sliku)\n",
        "* Nema moguÄ‡nost hijerarhijskog klasterovanja (razlikovanje viÅ¡e manjih podklastera unutar veÄ‡eg klastera)\n",
        "\n",
        "![img/2/kmeans_fail.png](img/2/kmeans_fail.png)\n",
        "\n",
        "---\n",
        "\n",
        "## DBSCAN\n",
        "\n",
        "DBSCAN (Density-based spatial clustering of applications with noise) je takoÄ‘e algoritam za klasterizaciju podataka. Ovaj postupak se zasniva na ideji grupisanja taÄaka (podataka) na osnovu njihove meÄ‘usobne udaljenosti. Ukoliko se taÄke nalaze u tzv. *epsilon okolini* one su deo nekog klastera, u suprotnom se posmatraju kao Å¡um.\n",
        "\n",
        "âš™ï¸ Opis DBSCAN algoritma:\n",
        "1. Neka postoji neki skup taÄaka (podataka) koje Å¾elimo da klasterizujemo. U samom postupku, razlikuju se tri vrste taÄaka: kljuÄne taÄke, dostupne taÄke i Å¡um.\n",
        "2. DBSCAN zahteva dva parametra: *epsilon* (eps) i *minimalni broj potrebnih taÄaka koje Äine region* (minPts). Epsilon okolina se najÄeÅ¡Ä‡e raÄuna koriÅ¡Ä‡enjem euklidske udaljenosti.\n",
        "3. Algoritam poÄinje sa proizvoljnom taÄkom. RaÄuna se epsilon okolina te taÄke, i ukoliko se u njoj nalazi dovoljno taÄaka (minPts), zapoÄinje se novi klaster. U suprotnom, taÄka se raÄuna kao Å¡um. Obratiti paÅ¾nju da taÄka, iako je Å¡um, kasnije *moÅ¾e* biti pronaÄ‘ena kao deo neke druge epsilon okoline sa dovoljno taÄaka i samim tim da postane deo klastera.\n",
        "4. Ukoliko je za taÄku odreÄ‘eno da pripada klasteru, sve taÄke u njenoj epsilon okolini takoÄ‘e pripadaju tom klasteru. Dakle, sve taÄke koje su pronaÄ‘ene u epsilon okolini trenutne taÄke se dodaju u klaster, kao i taÄke koje se nalaze u epsilon okolini tih taÄaka (rekurzivno). Proces se nastavlja dok se ne naÄ‘e ceo klaster, odnosno dok se ne obiÄ‘u sve taÄke u epsilon okolinama.\n",
        "5. Onda se nalazi nova, prozivoljna neposeÄ‡ena taÄka, za koju se ponavlja Äitav postupak, Å¡to dovodi do otkrivanja ili novog klastera ili Å¡uma.\n",
        "\n",
        "![img/2/dbscan.png](img/2/dbscan.png)\n",
        "\n",
        "---\n",
        "Obratite paÅ¾nju na prikaz sledeÄ‡eg skupa podataka:\n",
        "\n",
        "![img/2/data.png](img/2/data.png)\n",
        "\n",
        "Hajde da vidimo rezultat DB scan-a naspram KMeans za isti skup:\n",
        "\n",
        "![img/2/kmeans_vs_dbscan.png](img/2/kmeans_vs_dbscan.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_koBAfmewR0H"
      },
      "source": [
        "#### Prednosti DBSCAN\n",
        "\n",
        "* Nije potrebno unapred znati broj klastera (kao kod K-means)\n",
        "* Klasteri mogu biti proizvoljnog oblika\n",
        "* Ume da tretira Å¡um\n",
        "* Parametre epsilon i minPts je lako menjati u cilju dobijanja klastera razliÄitih veliÄina i oblika, i ove parametre Äesto podeÅ¡avaju eksperti sa domenskim znanjem\n",
        "\n",
        "\n",
        "#### Mane DBSCAN\n",
        "\n",
        "* Kvalitet rezultata zavisi od toga Äime se meri epsilon. ObiÄno je to euklidska udaljenost, ali za viÅ¡edimenzionalne podatke potrebne su drugaÄije metrike\n",
        "* Kada postoje varijacije u gustini klastera, nemoguÄ‡e je odrediti epsilon i minPts da odgovara svim klasterima\n",
        "* U sluÄaju kada ne postoji ekspert sa domenskim znanjem, odreÄ‘ivanje epsilon i minPts parametara je Äesto dosta teÅ¡ko\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "763p-NDIup1r"
      },
      "source": [
        "# Redukcija dimenzionalnosti"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSPNz0IPutIY"
      },
      "source": [
        "\n",
        "### ğŸŒŸ Analogija   \n",
        "\n",
        "Gledanje filma u 720p umesto 4K â€“ manje detalja, ali suÅ¡tina ostaje\n",
        "\n",
        "### ğŸ“š Definciija\n",
        "\n",
        "Redukcija dimenzionalnosti je proces smanjenja broja varijabli (osobina) u skupu podataka, pri Äemu se pokuÅ¡ava oÄuvati Å¡to viÅ¡e relevantnih informacija. MoÅ¾e se ostvariti metodama selekcije osobina (zadrÅ¾avanje najvaÅ¾nijih originalnih osobina) ili ekstrakcije osobina (kreiranje novih, reprezentativnih osobina)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_dZUVxe2OAI"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0QjBN6P2PH4"
      },
      "source": [
        "Principal Component Analysis (PCA) je tehnika koja se koristi za redukciju dimenzionalnosti podataka. Njena osnovna ideja je da pronaÄ‘e nove osobine podataka (tzv. glavne komponente) koje sadrÅ¾e Å¡to viÅ¡e informacije iz originalnih osobina, ali sa smanjenom dimenzionalnoÅ¡Ä‡u.\n",
        "\n",
        "ZaÅ¡to je korisno?\n",
        "1. Ubrzava algoritme i smanjuje memorijske resurse\n",
        "2. OlakÅ¡ava vizuelizaciju (npr. 3D â†’ 2D)\n",
        "3. Smanjuje Å¡um\n",
        "\n",
        "![img/2/pca.gif](img/2/pca.gif)\n",
        "\n",
        "---\n",
        "\n",
        "### âš™ï¸ Kako funkcioniÅ¡e PCA?\n",
        "\n",
        "PCA funkcioniÅ¡e tako Å¡to projektuje podatke u novu koordinatnu ravan, pri Äemu se bira pravac najveÄ‡e varijanse podataka. To se radi kroz sledeÄ‡e korake:\n",
        "\n",
        "1. Standardizacija podataka â€“ podatke treba normalizovati kako bi svi atributi imali isti znaÄaj (npr. skalirati ih da imaju srednju vrednost 0 i standardnu devijaciju 1).\n",
        "2. ProraÄun kovarijacione matrice â€“ odreÄ‘uje se kako su varijable meÄ‘usobno povezane.\n",
        "3. ProraÄun sopstvenih vrednosti i sopstvenih vektora â€“ sopstveni vektori predstavljaju pravce glavnih komponenti, dok sopstvene vrednosti odreÄ‘uju znaÄaj tih pravaca.\n",
        "4. Odabir glavnih komponenti â€“ bira se odreÄ‘en broj komponenti koje objaÅ¡njavaju najveÄ‡i deo varijanse podataka.\n",
        "\n",
        "Transformacija podataka â€“ originalni podaci se projektuju na nove glavne komponente.\n",
        "\n",
        "---\n",
        "\n",
        "##### Prednosti PCA\n",
        "\n",
        "* Smanjuje dimenzionalnost podataka, Äime se poboljÅ¡avaju performanse\n",
        "algoritama i smanjuje problem â€kletve dimenzionalnostiâ€.\n",
        "* ÄŒuva Å¡to je viÅ¡e moguÄ‡e informacije iz originalnih podataka uz minimalan gubitak.\n",
        "* Ubrzava treniranje modela jer smanjuje broj osobina.\n",
        "* Koristi se za vizualizaciju podataka (PCA moÅ¾e projicirati visoko-dimenzionalne podatke u 2D ili 3D prostor za lakÅ¡u interpretaciju).\n",
        "* Otkriva latentne strukture u podacima, omoguÄ‡avajuÄ‡i bolju interpretaciju.\n",
        "\n",
        "##### Mane PCA\n",
        "\n",
        "* MoÅ¾e izgubiti vaÅ¾ne informacije, posebno ako se izabere premali broj komponenti.\n",
        "* Nije idealan za nelinearne odnose â€“ PCA bazira transformaciju na linearnim kombinacijama osobina.\n",
        "* Osetljiv na skaliranje podataka â€“ loÅ¡e skalirani podaci mogu dovesti do loÅ¡ih rezultata.\n",
        "* TeÅ¾e interpretirati nove glavne komponente â€“ one Äesto nemaju oÄigledno znaÄenje u odnosu na originalne osobine.\n",
        "  \n",
        "![img/2/pca.png](img/2/pca.png)\n",
        "\n",
        "PCA (3D â†’ 2D):\n",
        " - Prva komponenta objaÅ¡njava 62.48% varijanse\n",
        " - Druga komponenta objaÅ¡njava 25.56% varijanse\n",
        "\n",
        "PCA (3D â†’ 1D):\n",
        " - Prva komponenta objaÅ¡njava 62.48% varijanse"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
